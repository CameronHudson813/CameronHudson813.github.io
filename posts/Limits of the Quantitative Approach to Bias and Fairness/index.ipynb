{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Limits of the Quantitative Approach to Bias and Fairness\n",
    "author: Cameron Hudson \n",
    "date: '2025-03-19'\n",
    "image: \"image.jpg\"\n",
    "description: Discussion of the efficacy of quantitative methods in evaluating the fairness of machine learning algorithms. \n",
    "format: html\n",
    "bibliography: refs.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This post explores the benefits and drawbacks of using quantitative methods to measure and detect discrimination. Quantitative approaches rely on mathematical definitions, machine learning algorithm auditing, and statistical tests to identify biases. However, @narayanan2022limits argues these methods often fail to capture the complexity of discrimination, leading to misleading conclusions and reinforcing existing inequalities. This post will examine Narayanan’s critique while also acknowledging cases where quantitative analysis has effectively exposed bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narayanan's Position\n",
    "Narayanan argues that quantitative methods are fundamentally inadequate for studying and addressing discrimination, as they often reinforce the status quo, which in turn leads to inaction due to the failure of the methods to detect discrimination. He begins his critique by highlighting a stark reality: “about 1% of Fortune 500 CEOs are Black” (@narayanan2022limits). Despite clear evidence of racial discrimination in the United States, the prevailing assumption—what Narayanan calls the “null hypothesis”—is that discrimination does not exist. This assumption shifts the burden of proof onto those who argue that racial disparities, such as the low number of Black Fortune 500 CEOs, are the result of systemic discrimination.  \n",
    "\n",
    "Narayanan further critiques quantitative methods by introducing the concept of compound inequality. He illustrates that even a seemingly small bias—such as a 2.5% advantage for white employees in quarterly performance reviews—can accumulate over time, leading to significant disparities in leadership representation. Crucially, he argues that such subtle, yet impactful biases are often undetectable by quantitative analysis, rendering these methods ineffective at uncovering the true extent of discrimination. Such an issue of incremental, long lasting discrimination being undetectable by quantitative methods is also addressed by Narayanan’s criticism of the data that is often used by quantitative methods themselves. Narayanan asserts that most datasets are snapshots, measurements of phenomenon in discrete and short points of time. Thus, it is incredibly difficult for these quantitative methods to capture discrimination, which is often incremental and persists for long periods of time.  \n",
    "\n",
    "Moreover, Narayanan argues that those in power can manipulate or selectively present data to misrepresent marginalized groups and reinforce false narratives. Narayanan also points out, such as the case of performance review bias in Fortune 500 companies, relevant data is excluded or extremely difficult to include in datasets, challenging the idea the discrimination can be measured in discrete quantitative snapshots and not through imperceivable events throughout the course of a person’s lifetime. It is with these points that Narayanan asserts that quantitative methods, being poor at detecting man forms of discrimination, are being utilized to report a lack of discrimination where there actually is discrimination, and thus feels that quantitative methods are doing more harm than good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benefits of Quantitative Methods\n",
    "Despite Narayanan’s criticisms, quantitative methods have played an essential role in uncovering systemic discrimination in machine learning algorithms. They excel in cases where disparities can be objectively measured. One notable example is ProPublica’s use of quantitative methods to reveal racial bias in Northpointe’s recidivism risk assessment algorithm (@angwin2022machine). Northpointe’s algorithm assigned risk scores to offenders, predicting their likelihood of reoffending. ProPublica’s audit revealed several critical flaws. The algorithm's positive predictive value was alarmingly low, with only 20% of those predicted to commit violent crimes actually doing so. Furthermore, the algorithm failed the error rate parity fairness criterion: Black defendants had significantly higher false positive rates (wrongly classified as high risk), while white defendants had disproportionately high false negative rates (wrongly classified as low risk). The audit demonstrated that not only was the algorithm a poor predictor of recidivism, but it also exhibited significant racial bias against Black defendants. Through quantitative analysis, ProPublica exposed how the algorithm generalized and prejudiced individuals based on race. For instance, it reported cases where white defendants with extensive criminal histories received lower risk scores than Black defendants with no prior arrests. By shifting the burden of proof onto Northpointe, quantitative methods played a key role in prompting a broader investigation into the moral implications of using flawed and biased algorithms in judicial decision-making. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Quantitative Methods\n",
    "Other scholars including Narayanan report the various limitations of quantitative methods in detecting discrimination. Narayanan, in his speech, specifically refers to ProPublica as being the cause of his disillusionment with the usefulness of quantitative methods. Narayanan expounds on his viewpoint referring to the developers of the COMPAS RPI in the quote, “If the developers of risk prediction algorithms redesigned them to equalize the rates of falsely flagging someone as high risk, between Black defendants and white defendants, that doesn’t solve the problem. It remains profoundly unjust to deny someone their freedom based on a prediction that they might commit another crime or a prediction that they might not appear in court for their arraignment or their trial”(@narayanan2022limits). The dilemma which Narayanan describes can be explained through description of the differences between the “broad view” and the “middle view” in @barocasFairnessMachineLearning2023. What Narayanan describes equalizing error rates would align with is be the “middle view”: the equalization of error rates in RPI’s and discriminatory systems refers to an enforcement of fairness at the decision-making level. What Narayanan believes is right, however, is more in line with the “broad view”. The broad view states that fairness should be addressed at the systemic level, and that the racial biases of RPI’s are a symptom of a larger problem. Through the example of ProPublica’s COMPAS audit, Narayanan demonstrates that even when quantitative methods can reveal injustice, they often do not provide the means or solutions to the root cause of those injustices. Within chouldechova2017fair,ProPublica's COMPAS audit is also criticized on its choice of fairness criteria, demonstrated in the quote, “Flores et al. [6] argue that the correct approach for assessing RPI bias is instead to check for calibration, a fairness criterion that they show COMPAS satisfies. Northpointe in their response[7] argue for a still different approach that checks for a fairness criterion termed predictive parity, which they demonstrate COMPAS also satisfies” (chouldechova2017fair). Chouldechova continues in his paper to prove algebraically that when prevalence differs among groups, the fairness criterions error rate parity and predictive parity can be satisfied simultaneously. The mathematical proof and reported push back of ProPublica’s audit in Chouldechova’s paper illustrate how quantitative methods can be insufficient in detecting real group bias as decision making systems can fulfill certain fairness criterion and not others. Other scholars report that the limits of quantiative methods lie in their reliance on incomplete or missing data. @d2023data highlight how Black women in the United States suffer disproportionately high maternal mortality rates. Yet, when researchers sought to investigate this disparity, they found a shocking reality—no reliable datasets existed to confirm or study the problem. As D’Ignazio and Klein note, “Nobody was counting.” A 2014 United Nations report described maternal mortality data collection in the United States as “particularly weak.” This example underscores a critical issue: quantitative methods become ineffective when the necessary data does not exist. The lack of relevant data often stems from the demographics of those responsible for collecting and analyzing it. D’Ignazio and Klein highlight that, as of 2018, only 26% of professionals in “computer and mathematical operations” were women, and only 12% of those were Black or Latinx women, despite these groups comprising 22.5% of the U.S. population.  \n",
    "\n",
    "This demographic imbalance creates a privilege hazard, where those in positions of power and influence—who are not personally affected by systemic discrimination—fail to recognize its existence, leading to the invisibility of marginalized experiences in data collection. Other scholars further critique the assumption that fairness can be adequately measured through mathematical models. The fairness of a machine learning system extends beyond technical accuracy and error rate parity; it also involves broader social and epistemological concerns. As one study points out, “The fairness of a data science project extends far beyond the technical properties of a given model and includes normative and epistemological issues that arise during processes of problem formulation, data collection, and real-world application.” In other words, quantitative metrics alone cannot account for the deeper structural inequalities that exist before any algorithm is even built. @hardtPatternsPredictionsActions2022 encapsulate this critique by arguing that discriminatory design does not require intentional malice. As he explains, “One need not harbor any racial animus to exercise racism... rather, when the default settings have been stipulated, simply doing one’s job...is enough to ensure the consistency of white domination over time.” This highlights how data-driven decision-making often reinforces existing social hierarchies, even in the absence of explicit bias.  \n",
    "\n",
    "This idea is further emphasized by @eubanks2018digital, who discusses in her article how automated systems are designed to perpetuate inequality and increase the ethical distance between human decision-makers, thereby justifying systemic bias. The article introduces the term “rational discrimination,” which serves as the justification and backbone of biased algorithms. Eubanks states, “Rational discrimination does not require class or racial hatred, or even unconscious bias, to operate. It requires only ignoring bias that already exists.” This aligns with the argument that data collection itself is a central issue in fairness for machine learning algorithms—one that cannot be addressed merely by measuring algorithmic outcomes across different groups. If the data does not accurately reflect the reality of discrimination or contains inherent biases, then no amount of technical auditing can uncover the deeper inequities that were embedded long before the algorithm was even created. This notion of “rational discrimination” aligns with Narayanan’s critique of quantitative methods, which he argues often serve to justify the status quo rather than challenge it.  \n",
    "\n",
    "In a fundamentally discriminatory system, rational discrimination operates within what @barocasFairnessMachineLearning2023 refer to as the “narrow view.” The narrow view asserts that people who are similar with respect to a task should be given similar opportunities and rewards, evaluating fairness purely at the level of the individual without considering how systemic discrimination shapes opportunities in the first place. This framework falsely presents itself as “meritorious” and “fair,” despite statistical realities suggesting otherwise—such as Narayanan’s observation that only 1% of Fortune 500 CEOs are Black. What rational discrimination fails to consider are the perspectives captured in the “middle view” and “broad view,” which acknowledge historical oppression and advocate for interventions that challenge systemic inequality. These perspectives call for active efforts to remedy disparities rather than passively accepting them as natural outcomes of a supposedly neutral process. However, such interventions are often absent from quantitative methods, as these issues extend beyond what numerical evaluations alone can address. As Narayanan explains, quantitative analyses frequently produce superficially plausible explanations that discourage further investigation or action. This is evident in his critique of the Fortune 500 CEO statistic: “As a quantitative scholar, you’re not allowed to conclude from this that there is discrimination in Fortune 500 companies. You’re supposed to be open to all possibilities. Like maybe Black people just aren’t that interested in becoming CEOs (@narayanan2022limits).” This is just one example of how quantitative methods, when used in isolation, fail to meaningfully challenge discrimination. By allowing for convenient yet flawed interpretations, these methods risk reinforcing existing disparities rather than prompting meaningful change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Point of View\n",
    "I agree with Narayanan’s assertion that quantitative methods are inherently limiting and often serve to deflect accusations of discrimination rather than substantiate them. While cases like ProPublica’s audit of Northpointe’s algorithm demonstrate that quantitative analysis can expose algorithmic bias, these methods become far less effective when oppressive structures manipulate data or when discrimination operates in ways that are not easily quantifiable. \n",
    "\n",
    "Quantitative approaches struggle with missing data, privileged perspectives in data collection, and the long-term accumulation of subtle biases. Furthermore, these methods are frequently used to absolve institutions of responsibility by maintaining the “null hypothesis” that discrimination does not exist unless proven with statistical certainty. This ignores the lived experiences of marginalized groups and the broader socio-political forces that shape inequality. \n",
    "\n",
    "While quantitative methods should not be abandoned entirely, they must be supplemented with qualitative research, historical context, and critical perspectives that account for the limitations of data-driven approaches. Discrimination cannot be reduced to a set of statistical tests; rather, it must be understood as a complex, systemic issue that requires a broader analytical lens. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "@barocasFairnessMachineLearning2023\n",
    "@narayanan2022limits\n",
    "@hardtPatternsPredictionsActions2022\n",
    "@eubanks2018digital\n",
    "@angwin2022machine\n",
    "@d2023data\n",
    "@chouldechova2017fair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "debe06cc0f9553f110b64dc3926c05df82dae2145b852c8422b9c04315589dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
