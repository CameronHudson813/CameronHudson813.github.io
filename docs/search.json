[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Cameron Hudson’s Machine Learning Blog",
    "section": "",
    "text": "A blog to explore machine learning!"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Automated Decision Systems/index.html",
    "href": "posts/Automated Decision Systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nToday, we are going to build a automated decision system that will be demonstrated using a dataset of borrowers from a bank. The objective of this decising making model is to use the training data to predicting whether a not a future applicant is likely to default a loan or repay it in full. We will use this model not only to demonstrate the creation of a decision making model, but also to assist a larger discussion on its societal impacts.\n\n\nGrabbing and Observing the Data\nFirst we import the dataset that includes previous borrowers:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\nWith df.head() we can see the features that are associated with each borrower:\n\ndf.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nBase Rates\nTo demonstrate that our machine learning model has indeed derived a pattern from the training data, we will try to achieve an accuracy above the base rate. The base rate is the accuracy that we can achieve if we predict one outcome for every possible decision.\n\n1-df[\"loan_status\"].mean()\n\nnp.float64(0.7824201964395334)\n\n\nin this case, loan_status equals 1 if the borrower did default on their loan, and 0 if they repaid it in full. df[\"loan_status\"].mean gets the percentage of borrowers that defaulted, and subtracting by 1 gives us the perctange of borrows that repaid the loan. If we always predict the applicant will repay the loan, then we will get correct roughly 78% of the time. In order to demonstrate learning by the model, we must aim for an accuracy above 78%.\n\n\nVisualizing the Data\nWe want to make informed decisions on the data, which requires that we search for patterns within that data. One question we may ask is: is there a observable pattern of loan intent with respect to age, length of employment, or homeownership status?\nTo discover this, we will make visualizations of our dataset to prove or dispprove the relation between age, length of employment, or homeownership status and loan intent. We will make use of the LabelEncoder in order to turn our values of loan intent (venture, education, home improvement) into values that will be easier to work with.\n\n\nSummary Table\n\nsummary_table = df.groupby(\"person_home_ownership\").agg({\n    \"loan_amnt\": \"mean\",\n    \"loan_int_rate\": \"mean\",\n    \"loan_status\": \"mean\" \n})\nsummary_table\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_status\n\n\nperson_home_ownership\n\n\n\n\n\n\n\nMORTGAGE\n10562.137462\n10.491245\n0.125058\n\n\nOTHER\n11235.795455\n12.059221\n0.306818\n\n\nOWN\n8978.912626\n10.850169\n0.080653\n\n\nRENT\n8843.507973\n11.448571\n0.313971\n\n\n\n\n\n\n\n\n\nDiscussion\nThis summary table displays the mean loan amount, loan interest rate, and the rate of defaulting the loan based on the person’s home ownership status. We can see that the loan interest rate is about the same for every home ownership type, around 10-12%, as well as the loan amount, however the default rate differs quite dramatically between groups. Those who own their homes statistically almost never default, however those who are renting or have their home ownership status as “OTHER”, have a default rate around 30%. People in the dataset with their home ownership status as “mortgage” have a default rate lower at 12%.\n\n\nScatterplot 1 using loan_amnt and loan_int_rate\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(figsize = (8, 3.5))\n\np2 = sns.scatterplot(df, x = \"loan_amnt\", y = \"loan_int_rate\", ax = ax, hue = \"loan_status\", palette=\"Set2\")\n\n\n\n\n\n\n\n\n\n\nPatterns in the Data\nWe can discern from this visualization that in this dataset, whether a borrower defaults or not is not very dependent on the loan amount, but their interest rate, where borrowers with higher interest rates, usually greater than 13%, are more likely to default than borrowers with interest rates below 13%.\n\n\nScatter plot 2 using cb_person_default_on_file\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(figsize = (8, 3.5))\n\np2 = sns.scatterplot(df, x = \"loan_amnt\", y = \"loan_int_rate\", ax = ax, hue = \"cb_person_default_on_file\", palette=\"Set2\")\n\n\n\n\n\n\n\n\n\n\nPatterns in the Data\nHere we see a similar split between the data, whereby borrowers who have previously defaulted on their loans have higher interest rates then those who payed back their previous loan in full. As we have seen from the last visualization that interest rates are corellated with the rate of defaulting the loan, we may want to consider if they have previously defaulted in our prediction.\n\n\nBuilding the Prediction Model\nIn order to build our model, we are first going to have to prepare our data to support accurate prediction.\n\n\nData Processing\nOur first step to processing the data to be trained is to drop unnessecary rows and columns. In this instance, we must drop all rows that do not contain a value for one of their columns, as well as our target variable loan status and loan grade. We use X = pd.get_dummies(X) to hot encode our categorical features such as loan_intent, person_home_ownership, and cb_person_default_on_file, store our target variable in the variable y, and use sklearn’s train_test_split to split the data into training and testing data for cross validation.\n\nfrom sklearn.model_selection import train_test_split\n\ndef prepare_data(df):    \n    df = df.dropna()\n    X = df.drop([\"loan_grade\", \"loan_status\"], axis=1)  \n    X = pd.get_dummies(X)\n    y = df[\"loan_status\"]\n    return X, y\nX, y = prepare_data(df)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n21383\n35\n50532\n8.0\n5000\n13.57\n0.10\n8\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n2116\n25\n60000\n5.0\n1200\n13.85\n0.02\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n5631\n30\n55000\n1.0\n6500\n10.99\n0.12\n7\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n20675\n30\n34046\n2.0\n12000\n14.96\n0.35\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n9582\n23\n70522\n7.0\n9325\n10.91\n0.13\n3\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13577\n32\n52000\n0.0\n2500\n8.88\n0.05\n10\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n24532\n36\n60000\n7.0\n17050\n12.69\n0.28\n14\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6125\n22\n27264\n4.0\n5000\n13.11\n0.18\n3\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n974\n35\n50000\n0.0\n5600\n12.53\n0.11\n7\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17904\n28\n47000\n5.0\n9000\n12.42\n0.19\n8\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n18325 rows × 19 columns\n\n\n\n\n\nPicking the Features and Fitting the Model\nWe will be using sklearn’s SelectKBest with the scoring function mutual_info_classif to get the most predictive features, limiting the number of features to 6 to lower the amount of iterations the machine learning model must compute.\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nselector = SelectKBest(score_func=mutual_info_classif, k=6)\nX_selected = selector.fit_transform(X_train, y_train)\n\nfeatures = X.columns[selector.get_support()]\n\nprint(features)\n\nIndex(['person_income', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_RENT', 'cb_person_default_on_file_N',\n       'cb_person_default_on_file_Y'],\n      dtype='object')\n\n\n\n\nBuilding the Model\nNow that we have obtained our features, we create a logistic regression model and train it on X_train, using only our selected models with X_train[features]. After fitting the model, we can produce a score that will show our models predictive accuracy.\n\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(max_iter=50000)\nLR.fit(X_train[features], y_train)\nLR.score(X_train[features], y_train)\n\n0.8461664392905867\n\n\n\n\nResults\nWe see through our score that we got a higher predictive accuracy than our base rate of 78%, showing that are model indeed learned patterns through our training data and selected features.\n\nLR.score(X_test[features], y_test)\n\n0.8384984722828459\n\n\nWe get similar results on new data, showing that are model was able to create generalizations that can be extrapolated to predict unseen data.\n\n\nOur Goal\nOur goal now is to find a threshold that will provide a binary answer to the question of whether our not the bank should give a loan to a potential appliant and to compute the overall profit gained or lost by the bank.\n\n\nApproach\nThe apporach we will be taking is to first calculate a score to give each applicant s, with that score being the likelihood of whether or not an applicant will default on their loan or not. If this score is higher than our choosen threshold, the model will predict that the applicant will default on the loan and the bank will not lend to the applicant. If this score is lower or equal to our choosen threshold, the bank will lend\n\n\nCalculations\nIn order to evaluate the total profit that the bank gains or losses given a particular threshold value, we will assume for convience that if the loan is repaid in full, the profit for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt and if the borrower defaults on the loan, the “profit” for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt. If the bank doesn’t give out a loan, their profit is 0 for that particular borrower.\n\n\nWeights\nOur weights, stored in the variable in LR.coef where LR is our logistic regression model shows an array with all of the weights for our chosen features respectively. This is also known as our weight vector, which we can then use to find the threshold in order for our model to produce a yes or no answer.\n\nweight_vector = LR.coef_\nweight_vector\n\narray([[-9.48057341e-06,  2.85958980e-01,  7.67127417e+00,\n         1.01511915e+00, -2.08025653e+00, -1.98039962e+00]])\n\n\n\n\nFinding a Threshold\nIn order to create an automated system that finds the threshold that maximizes the total profit, we are going to split this task up into three functions: linear_score, individual_profit, and find_threshold.\n\n\nLinear Score Function\nThis function computes the dot product of the values of the features for each borrower in the dataset and multiple them by the weights determined by the logistic regression model. This in turn will produce a score which gives a numerical value to the likelihood of the borrower defaulting or repaiding a loan.\n\ndef linear_score(features, weights):\n    return features@weights.flatten()\n\n\n\nIndividual Profit Function\nThis function takes the loan amount and loan intrest rate of a borrower and computes based on their score and the threshold the profit that the bank will make dealing with this applicant. If the score is greater than our chosen threshold, the bank’s profit will be 0, because the bank will not lend to the potiental borrower. In the case where the model predicts the applicant will repay the loan, we then are faced with two more options. If the applicant actaully did repay the loan, that the profit is the repaid_profit value discussed previously. If the bank lends to an applicant but they default, then the bank in this case will lose profit, shown by the value of default_profit.\n\nimport numpy as np\ndef individual_profit(loan_amnt, loan_int_rate, score, threshold, loan_status):\n        loan_int_rate = loan_int_rate / 100\n        repaid_profit = loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt\n        defaulted_profit = loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt\n        return np.where(score &lt;= threshold, \n                    np.where(loan_status, defaulted_profit, repaid_profit), \n                    0)\n\n\n\nFind Threshold Function\nThis function uses np.linspace(0,1,1000) to enumerate through 1000 threshold values evenly spaced between 0 and 1, and notes the total profit gained from that selected threshold with the value of total_proft using the np.sum function. If that profit is greater than our value for max_profit, then we update max_profit and best_threshold to reflect the threshold value that gives the bank the maximum profit.\n\nimport numpy as np\ndef find_threshold(df, features, score_function, w, y):\n    max_profit = float(\"-inf\")\n    best_threshold = 0\n    scores = score_function(df[features], w)\n    thresholds = np.linspace(0, 1, 1000)\n    for threshold in thresholds:\n        total_profit = np.sum(individual_profit(df[\"loan_amnt\"], df[\"loan_int_rate\"], scores, threshold, y))\n        if total_profit &gt; max_profit:\n            max_profit = total_profit\n            best_threshold = threshold\n\n    return best_threshold, max_profit\n    \n\n\n\nResults\nOur resulting function reports that the best threshold value is 0.99 which will predict a profit of $280,747.\n\nthreshold, profit = find_threshold(X_train, features, linear_score, weight_vector, y_train.values)\n(threshold, profit)\n\n(np.float64(1.0), np.float64(6790404.549188839))\n\n\n\n\nProfit per Borrower\nWe can then compute how much money on average the bank generates for each borrower using this automated decision making system by dividing the total profit by the number of borrowers in the dataset.\n\nprofit_per_borrower = profit / len(X_train)\nprofit_per_borrower\n\nnp.float64(370.55413638138276)\n\n\n\n\nModel Evaluation: Bank’s Perspective\nTo sum up our creation of a predictive decision making model, we are going to test it on the data stored in df_test.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\nX_df_test, y_df_test = prepare_data(df_test)\nLR.fit(X_df_test[features], y_df_test)\nweight_vector = LR.coef_\nthreshold, profit = find_threshold(X_df_test, features, linear_score, weight_vector, y_df_test.values)\nthreshold, profit\n\n(np.float64(1.0), np.float64(112120.47474005715))\n\n\nRunning our automated decision making system on the\n\n\nModel Evaluation: Borrower’s Perspective\nLets now evaluate how the model treats individual borrowers based on their age, loan intent and income. the main questions we want to ask are those of discrimination: Is it more difficult for people in certain age groups to access credit? Is it more difficult for people to get loans in order to pay for medical expenses? How does a person’s income level impact the ease with which they can access credit?\n\n\nDifferences in Age Groups Access To Credit\nTo answer this question we can make a summary table. We use the pd.cut function to make continous variables into categorical variables.\n\nimport pandas as pd\nimport numpy as np\n\nage_bins = [18, 25, 35, 50, 65, 100]\nage_labels = [\"&lt;25\", \"25-35\", \"35-50\", \"50-65\", \"&gt;65\"]\n\ndf_test[\"age_group\"] = pd.cut(df_test[\"person_age\"], bins=age_bins, labels=age_labels)\n\nscores = linear_score(X_df_test[features], weight_vector)\n\ndf_test[\"approved\"] = scores &gt; threshold\n\nage_acceptance_rates = df_test.groupby(\"age_group\")[\"approved\"].mean()\ndisplay(age_acceptance_rates)\n\n/var/folders/pv/gfxr_25s0q9gt_2k6h_bbgmc0000gn/T/ipykernel_31580/1654173622.py:16: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  age_acceptance_rates = df_test.groupby(\"age_group\")[\"approved\"].mean()\n\n\nage_group\n&lt;25       0.99521\n25-35    0.987067\n35-50    0.979094\n50-65     0.97561\n&gt;65           1.0\nName: approved, dtype: object\n\n\n\n\nResults\nWhat we can see from the function’s output is that people higher in age (likely correlated with higher income) are more likely of being lent a loan.\n\n\nDifficuly Getting Loans for Medical Expenses\nTo answer this question, we will use the pd.groupby() function to group all of the applicants in df_test into the two groups loan_intent_acceptance which contains only the applicants that got their loan request approved and gets the percentage of those applicants whos loan intent was one of the following loan_intent values, and loan_intent_default which the same as loan_intent_acceptance except that it contains the true rate of default for that group.\n\nloan_intent_acceptance = df_test.groupby([\"loan_intent\"])[\"approved\"].mean()\n\n\nloan_intent_default = df_test.groupby([\"loan_intent\"])[\"loan_status\"].mean()  \n\nloan_analysis = pd.DataFrame({\n    \"Acceptance Rate\": loan_intent_acceptance,\n    \"Default Rate\": loan_intent_default\n})\ndisplay(loan_analysis)\n\n\n\n\n\n\n\n\nAcceptance Rate\nDefault Rate\n\n\nloan_intent\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.990044\n0.279497\n\n\nEDUCATION\n0.988946\n0.167421\n\n\nHOMEIMPROVEMENT\n0.993506\n0.246088\n\n\nMEDICAL\n0.991612\n0.281553\n\n\nPERSONAL\n0.988978\n0.219227\n\n\nVENTURE\n0.988589\n0.145701\n\n\n\n\n\n\n\n\n\nIncome Level vs Access to Credit\nTo answer this question we take the same approach we took in tacking difference in loan availibility based on age groups to answer the question based on income groups.\n\nincome_bins = [0, 30000, 60000, 100000, 500000]\nincome_labels = [\"&lt;30K\", \"30K-60K\", \"60K-100K\", \"&gt;100K\"]\n\ndf_test[\"income_group\"] = pd.cut(df_test[\"person_income\"], bins=income_bins, labels=income_labels)\n\nincome_acceptance_rates = df_test.groupby(\"income_group\")[\"approved\"].mean()\ndisplay(income_acceptance_rates)\n\n/var/folders/pv/gfxr_25s0q9gt_2k6h_bbgmc0000gn/T/ipykernel_31580/2528908447.py:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  income_acceptance_rates = df_test.groupby(\"income_group\")[\"approved\"].mean()\n\n\nincome_group\n&lt;30K             1.0\n30K-60K          1.0\n60K-100K         1.0\n&gt;100K       0.998652\nName: approved, dtype: object\n\n\n\n\nResults\nHere, we see that applicants who have a higher income have a much higher chance of getting a loan request accepted then applicants with lower incomes.\n\n\nOverall Reflection\nHere we highlight notable problems with a decision making algorithm deciding whether or not applicants deserve loans or not. As the model, weights, and threshold were decided with the intention of garnering the bank with the maximum amount of profit, there were considerable issues regarding how the model treated individual applicants. My findings note that the need for the loan (due to lower age, income, or important reasons such as a medical expenses) is inversely related to the availability of the loan under this decision making system."
  },
  {
    "objectID": "posts/Penguin Blog Post/index.html",
    "href": "posts/Penguin Blog Post/index.html",
    "title": "Palmer Penguins Blog Post",
    "section": "",
    "text": "Welcome! Today, I am going to demonstrate how to train a Logistic Regression model in order to make predictions on species of penguins using the Palmer Penguins dataset. We will cover how to find visualize and select good qualitative and quantiative predictors, bruteforce feature selection, how to plot decision regions, confusion matrices in attempt to show a comprehensive guide of the process of making a predictor model with a dataset.\n\nAcquring the Data\nWe are first going to import two popular data science python libraries, pandas and numpy. After this, we are going to get our Palmers Penguins dataset from the url listed below, and use pandas to convert the csv file into a DataFrame which will be stored in the variable train.\n\nimport pandas as pd\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nLet’s see 5 of the pieces of data we are working with using the head() method.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nData Preperation\nWe are now going to prepare the data. Here we drop columns that are likely unhelpful in making a prediction on the species of penguins, specifying axis = 1 to delete the column instead of the index. We then remove all rows were the value of df[“Sex”] is undefined or “.”. We use the method dropna() to drop all the rows that contain missing values. We then use the method fit() from the LabelEncoder object to pick numerical values from qualitative values. For example, our penguin species are “Gentoo”, “Chinstrap”, and “Adelie” which could be encoded as 0, 1, and 2. Now that we have encoded the values of the “Species” column, we create a column that uses that encoding with le.transform, and we want to use the encoded values on the “Species” column, so we will do le.transform(df[“Species”]) and store this new column in y. We will then drop the original Species column as we will not need it for our prediction. Finally, we do pd.get_dummies(df) which will convert all categorical variables that only have 2 unique values to a 0 or a 1. With this, we have prepared are data and are ready to use it to make predictions on species of Penguins.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nfor i, c in enumerate(le.classes_):\n    print(f\"Class number {i} represents {c} penguins.\")\n\nX_train, y_train = prepare_data(train)\n\nClass number 0 represents Adelie Penguin (Pygoscelis adeliae) penguins.\nClass number 1 represents Chinstrap penguin (Pygoscelis antarctica) penguins.\nClass number 2 represents Gentoo penguin (Pygoscelis papua) penguins.\n\n\nHere, we also enumerated through the classes made by the LabelEncoder and used a fstring to cleanly show which number encoding referes to which penguin.\nNow we can see that we have removed unnessecary columns, dropped rows with empty values, and changed qualitative columns to columns of True or False (1 or 0) values:\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nData Visualizations\nHere we are going to visualize the relationship between some quantative data to get a feel for what features might be useful for our predictor model.\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nle.classes_ = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nspecies_names = [le.classes_[i] for i in y_train]\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(X_train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(X_train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", ax = ax[1], hue = species_names, palette=\"Set2\")\n\n\n\n\n\n\n\n\nHere we can see that the relationship between Flipper Length and Body Mass is good for distinguishing between Gentoo penguins and the other two species (data shows that Gentoo penugins on average have a greater body mass and flipper length than the two other species), however it is not so good for dintinguishing between the Adelie and Chinstrap penguin species.\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[1], hue = species_names, palette=\"Set2\")\n\n\n\n\n\n\n\n\nWe can see here that utilizing the culmen length and the flipper length gives rather distinct groups where each species of penguins resides. While some points overlap, showing that these variables don’t 100% identify a penguin’s species, it is certainly better at identifying chinstrap and adelie penguins from the last set of variables.\n\n\nSummary Tables and Aggregate Values\nBeyond plotting which is good for visualizing quantiative features and there usefulness, we can use aggregate values to gain more information on quantiative features such as sex, island, and clutch completion.\n\ntrain.groupby(\"Clutch Completion\").size() / X_train.shape[0]\n\nClutch Completion\nNo     0.117188\nYes    0.957031\ndtype: float64\n\n\n\ntrain.groupby(\"Sex\").size() / X_train.shape[0]\n\nSex\n.         0.003906\nFEMALE    0.519531\nMALE      0.511719\ndtype: float64\n\n\n\ntrain.groupby(\"Island\").size() / X_train.shape[0]\n\nIsland\nBiscoe       0.511719\nDream        0.398438\nTorgersen    0.164062\ndtype: float64\n\n\nHere we can see the percentage of all the penguins split between the various qualitative features (Sex, Island, and Clutch Completion). With this we can see that clutch completion will probably not help us determine the species of a particular penguin as for 95% of the penguins in the dataset Clutch_Completion is true. On the other hand, it seems that half of the penguins are male and the other half female, and that a penguin’s location on one of the three islands is also quite varied, making these qualiative features more useful for predictions than cluch completion.\n\n\nFeature Selection\nOur current task now is to find three of these features listed where 2 are quantiative and 1 are quantitative that will predict with high accuracy the species of Penguin (either Gentoo, Chinstrap, or Adelie).\n\nprint(X_train.columns)\n\nIndex(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)',\n       'Island_Biscoe', 'Island_Dream', 'Island_Torgersen',\n       'Stage_Adult, 1 Egg Stage', 'Clutch Completion_No',\n       'Clutch Completion_Yes', 'Sex_FEMALE', 'Sex_MALE'],\n      dtype='object')\n\n\nBecause we have a relatively small amount of features, we are going to train a logistic regression model on every distinct combination of 2 quantitative feautures and 1 qualitative feature, and keep track in the variable max_score what combination of features (stored in best_features) achieves the highest acurracy.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\ntest_cols = X_train\nbest_features = ['None']\nmax_score = 0\n\nLR = LogisticRegression(max_iter=20000)\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    test_cols = X_train[cols]\n    m = LR.fit(test_cols, y_train)\n    score = LR.score(test_cols, y_train)\n    if score &gt; max_score:\n      max_score = score\n      best_features = cols\nbest_features = best_features[2:] + best_features[0:2]\nm = LR.fit(X_train[best_features], y_train)\nprint(max_score, best_features)\n\n0.99609375 ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\nWith this, we have discovered that training a logistic regression model on the qualitative feature Sex (Male or Female) and the quantitative features Culmen Length (mm) and Culmen Depth (mm) gives the highest prediction accurracy among all the other 3 feature combinations with a accuracy of 99.5% on the training data.\n\n\nTesting the model on unseen data\nWe have achieved high accuracy using the training data, but can we achieve similar results using a different subset of data?\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_features], y_test)\n\n0.9852941176470589\n\n\nWe can see that the accuracy of our model on new data is only slightly worse at 98.5%.\n\n\nPlotting Decision Regions\nWe can more explicitly display how our model is deciding the species of penguins with our selected features with this plot_regions function.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nWe are first going to run this using the training data:\n\nplot_regions(LR, X_train[best_features], y_train)\n\n\n\n\n\n\n\n\nThen, we are going to run it through the testing data:\n\nplot_regions(LR, X_test[best_features], y_test)\n\n\n\n\n\n\n\n\nHere we can see that the distinct regions that model predicted each of the 3 species of penguins would be located at mostly correlates with the actual positions and species of the penguin data points, giving a more visual representation of our accuracy value of 99.6% on training data and 98.5% on testing data.\n\n\nConfusion Matrix\nUsing the confusion_matrix function from sklearn, we can create a matrix that shows the accurate and inaccurate predictions of our model.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[best_features],)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 10,  1],\n       [ 0,  0, 26]])\n\n\nHere, on the diagonal of the matrix is the correct guesses, with each row corresponding to each possible label prediction (Chinstrap, Adelie, or Gentoo). We can make the output of our confusion matrix more appearent by returning to our LabelEncoder and looping through its contents:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 10 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 1 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nWe can see that the only error the model made on the testing data was that it misidentified a Chinstrap penguin for a Gentoo penguin. This correlates with the plot_regions output using the training data, as the points that represent Chinstrap penguins were often close or in the region that predicts a penguin will be a Gentoo penguin.\n\n\nClosing\nWith this, we have predicted with high accuracy the species of penguins within the Palmer Penguins dataset. Through the process of feature selection and plotting data we found that the sex, culmen length and depth of penguins within the dataset are the best features to predict the particular species of that penguin. This exercise has taught me the general workflow of creating a predictor model with sklearn, numpy and pandas from start to finish. I also learned techniques of plotting data (scatterplots, desicion regions) what a confusion matrix is and how to interpret it, and the process of feature selection."
  },
  {
    "objectID": "posts/tests/index.html",
    "href": "posts/tests/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/tests/index.html#math",
    "href": "posts/tests/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Today, we will be using the PUMS dataset to predict employment of members in the state of Michigan that are within the dataset, as well as auditing for racial bias. The process we will be using is first implementing a SVC to predict employment status based on features excluding race and the target variable (employment status), then evaluating our findings based on statistical definitions of fairness, such as calibration, error rate balance, and statistical parity of each racial group (white, black, or other). Through this process, I evaluated my model as passing both approximate cabliration and error rate balance tests but failing the statistical parity test, with white people having a higher rate of being predicted to be employed than black people or other people of a self-identified racial identity. Overall, I note in the conclusion the predicted impact of implementing this predictor model at large and note potential issues/uncomforabilites."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#overall-measures",
    "href": "posts/Auditing Bias/index.html#overall-measures",
    "title": "Auditing Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\n\nWhat is the overall accuracy of the model?\nWhat is the positive predictive value (PPV) of the model?\nWhat are the overall false negative and flase positive rates (FNP and FPR) for the model?\n\n\nOverall Acurracy of Model\nTo get the overall accuracy of the model, get a boolean vector through (y_hat == y_test) which will give an array of truth or false values corresponding to the equality of the prediction and the actual value. To get the percentage of the correct predictions in the boolean vector, we take the mean with the mean() function.\n\nmodel_accuracy = (y_hat == y_test).mean()\nmodel_accuracy\n\nnp.float64(0.825940454636894)\n\n\nThe overall accuracy of the model is approximately 82.5%\n\n\nPPV of the Model\nTo get the positive predictive value of the model, we divide the number of true positives (TP) by the total number of positive predictions (TP + FP)\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_array = confusion_matrix(y_test, y_hat)\nconfusion_array\n\narray([[8744, 2269],\n       [1192, 7679]])\n\n\n\nTP = confusion_array[0][0]\nFN = confusion_array[0][1]\nFP = confusion_array[1][0]\nTN = confusion_array[1][1]\n\nPPV = TP / (TP+FP)\nPPV\n\nnp.float64(0.8800322061191627)\n\n\n\n\nFPR and FNR of the Model\nSimilarly, we can use the confusion matrix to get the FPR and FNR of the model\n\n# False positive rate calculation\nFPR = FP / (FP + TN)\nFPR\n\nnp.float64(0.1343704204711983)\n\n\n\n# False negative rate calculation\nFNR = FN / (FN + TP)\nFNR\n\nnp.float64(0.2060292381730682)"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#by-group-measures",
    "href": "posts/Auditing Bias/index.html#by-group-measures",
    "title": "Auditing Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\n\nWhat is the accuracy of your model on each subgroup\nWhat is the PPV of your model on each subgroup?\nWhat are the FNR and FPR on each subgroup?\n\nTo answer question 1, we can make a summary table displaying the accuracy of the model on each subgroup\n\naccuracy_group_1 = (y_hat[group_test == 1] == y_test[group_test == 1]).mean()\naccuracy_group_2 = (y_hat[group_test == 2] == y_test[group_test == 2]).mean()\naccuracy_group_3 = (y_hat[group_test == 3] == y_test[group_test == 3]).mean()\n\nsubgroup_accuracy = pd.Series({\"White\": accuracy_group_1,\"Black\": accuracy_group_2, \"Other\": accuracy_group_3})\nsub_acc_df = pd.DataFrame(subgroup_accuracy, columns=[\"Accuracy\"])\nsub_acc_df\n\n\n\n\n\n\n\n\nAccuracy\n\n\n\n\nWhite\n0.827000\n\n\nBlack\n0.816878\n\n\nOther\n0.793651\n\n\n\n\n\n\n\nTo answer question 2, we can return to the idea of confusion matrices, making one for each subgroup and calculating the PPV as TP (True Positive Predictions) / TP + FP (False Positive Predictions).\n\nwhite_confusion_array = confusion_matrix((y_test[group_test == 1]), y_hat[group_test == 1])  \nblack_confusion_array = confusion_matrix((y_test[group_test == 2]), y_hat[group_test == 2])\nother_confusion_array = confusion_matrix((y_test[group_test == 3]), y_hat[group_test == 3])\n\nwhite_PPV = white_confusion_array[0][0] / (white_confusion_array[0][0] + white_confusion_array[1][0])\nblack_PPV = black_confusion_array[0][0] / (black_confusion_array[0][0] + black_confusion_array[1][0])\nother_PPV = other_confusion_array[0][0] / (other_confusion_array[0][0] + other_confusion_array[1][0])\n\ngroup_PPV = pd.Series({\"white\": white_PPV,\"black\": black_PPV, \"other\": other_PPV})\ngroup_PPV_df = pd.DataFrame(group_PPV, columns=[\"PPV\"])\ngroup_PPV_df\n\n\n\n\n\n\n\n\nPPV\n\n\n\n\nwhite\n0.877176\n\n\nblack\n0.899512\n\n\nother\n0.906250\n\n\n\n\n\n\n\nTo answer question 3, we can take a simiar approach as shown above, only switching the values that we are taking from the confusion matrix\n\nwhite_confusion_array = confusion_matrix((y_test[group_test == 1]), y_hat[group_test == 1])  \nblack_confusion_array = confusion_matrix((y_test[group_test == 2]), y_hat[group_test == 2])\nother_confusion_array = confusion_matrix((y_test[group_test == 3]), y_hat[group_test == 3])\n\nwhite_FNR = white_confusion_array[0][1] / (white_confusion_array[0][1] + white_confusion_array[0][0])\nwhite_FPR = white_confusion_array[1][0] / (white_confusion_array[1][0] + white_confusion_array[1][1])\n\nblack_FNR = black_confusion_array[0][1] / (black_confusion_array[0][1] + black_confusion_array[0][0])\nblack_FPR = black_confusion_array[1][0] / (black_confusion_array[1][0] + black_confusion_array[1][1])\n\nother_FNR = other_confusion_array[0][1] / (other_confusion_array[0][1] + other_confusion_array[0][0])\nother_FPR = other_confusion_array[1][0] / (other_confusion_array[1][0] + other_confusion_array[1][1])\n\ngroup_FNR = pd.Series({\"white\": white_FNR,\"black\": black_FNR, \"other\": other_FNR})\ngroup_FPR = pd.Series({\"white\": white_FPR,\"black\": black_FPR, \"other\": other_FPR})\n\n\n\ngroup_FPR_FNR_df = pd.DataFrame({\"FNR\": group_FNR, \"FPR\": group_FPR})\ngroup_FPR_FNR_df\n\n\n\n\n\n\n\n\nFNR\nFPR\n\n\n\n\nwhite\n0.208654\n0.130886\n\n\nblack\n0.198957\n0.155589\n\n\nother\n0.256410\n0.125000"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#bias-measures",
    "href": "posts/Auditing Bias/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\n\nIs the model approximately calibrated?\nDoes the model satisfy approximate error rate balance?\nDoes the model satisfy statistical parity?\n\nTo answer question 1, we can get the calibration of the model by evulating it by group. If the model is approximately calibrated, then its accuracy at predicting in each group * people in the dataset should roughly equal the number of the people of that particular group in the dataset.\n\n(int(X_test.shape[0] * accuracy_group_1), X_test[group_test == 1].shape[0]), \n\n((16444, 16815),)\n\n\n\n(int(X_test.shape[0] * accuracy_group_2), X_test[group_test == 2].shape[0])\n\n(16242, 1813)\n\n\n\n(int(X_test.shape[0] * accuracy_group_3), X_test[group_test == 3].shape[0])\n\n(15780, 126)\n\n\nWe can see that for group 1 the model is highly calibrated however for groups 2 and 3 the model is not.\nTo answer question 2, we must see if the true positive and false positive rates between the three groups are roughly equal. As we have done before, we display a dataframe that will allow us to analyze the relationship of the true postive and false positive rates between groups. Because we have already computed the false positive rate for each group, we only need to compute the true positive rate.\n\nwhite_confusion_array = confusion_matrix((y_test[group_test == 1]), y_hat[group_test == 1])  \nblack_confusion_array = confusion_matrix((y_test[group_test == 2]), y_hat[group_test == 2])\nother_confusion_array = confusion_matrix((y_test[group_test == 3]), y_hat[group_test == 3])\n\nwhite_TPR = white_confusion_array[0][0] / (white_confusion_array[0][0] + white_confusion_array[0][1])\n\nblack_TPR = black_confusion_array[0][0] / (black_confusion_array[0][0] + black_confusion_array[0][1])\n\nother_TPR = other_confusion_array[0][0] / (other_confusion_array[0][0] + other_confusion_array[0][1])\n\ngroup_TPR = pd.Series({\"white\": white_TPR,\"black\": black_TPR, \"other\": other_TPR})\n\n\n\ngroup_TPR_FPR_df = pd.DataFrame({\"TPR\": group_TPR, \"FPR\": group_FPR})\ngroup_TPR_FPR_df\n\n\n\n\n\n\n\n\nTPR\nFPR\n\n\n\n\nwhite\n0.791346\n0.130886\n\n\nblack\n0.801043\n0.155589\n\n\nother\n0.743590\n0.125000\n\n\n\n\n\n\n\nWe can see that between the 3 groups, the TPR and FPR are roughly the same, with the TPR for each group around 75-56% and the FPR around 13%.\nTo answer question 3 and evaluate the model for statistical parity, we must evaluate if the probability of positive prediction (this person is predicted to be employed) is the same across the different groups (black, white, other).\n\nprob_white = X_test[(group_test == 1) & (y_hat == 1)].shape[0] / X_test[group_test == 1].shape[0] \nprob_black = X_test[(group_test == 2) & (y_hat == 1)].shape[0] / X_test[group_test == 2].shape[0] \nprob_other = X_test[(group_test &gt;= 3) & (y_hat == 1)].shape[0] / X_test[group_test &gt;= 3].shape[0] \nprob_series = pd.Series({\"white\": prob_white, \"black\": prob_black, \"other\": prob_other})\nprob_df = pd.DataFrame(prob_series, columns=[\"Probability of Employed Prediction\"])\nprob_df\n\n\n\n\n\n\n\n\nProbability of Employed Prediction\n\n\n\n\nwhite\n0.511448\n\n\nblack\n0.434639\n\n\nother\n0.445860\n\n\n\n\n\n\n\nWe can see here that model does not pass the test of statistical parity, as the black and other groups have roughly the same percentage, however the white group is ahead of both by roughly 7 percent.\nWe will conclude our visual demonstration of the fairness of this model by\nWe will conclude our visual demonstration of the fairness of this model by\n\nimport seaborn as sns\nimport matplotlib.pyplot as pyplot\n\n#FPR and FNR for each group\nFPR_groups = [white_FPR, black_FPR, other_FPR]\nFNR_groups = [white_FNR, black_FNR, other_FNR]\n\n#prevalance percentage for each group\nwhite_p = X_test[(group_test == 1) & (y_test == 1)].shape[0] / X_test[group_test == 1].shape[0]\nblack_p = X_test[(group_test == 2) & (y_test == 1)].shape[0] / X_test[group_test == 2].shape[0]\nother_p = X_test[(group_test &gt;= 3) & (y_test == 1)].shape[0] / X_test[group_test &gt;= 3].shape[0]\n \ngroup_PPV = min(white_PPV, black_PPV, other_PPV)\n\n#line of feasible FPR and FNR values for each group\nfnr_values = np.linspace(0, 1, 100)\nwhite_fpr_values = (white_p / (1 - white_p)) * ((1 - group_PPV) / group_PPV) * (1 - fnr_values)\nblack_fpr_values = (black_p / (1 - black_p)) * ((1 - group_PPV) / group_PPV) * (1 - fnr_values)\nother_fpr_values = (other_p / (1 - other_p)) * ((1 - group_PPV) / group_PPV) * (1 - fnr_values)\n\npyplot.figure(figsize=(9, 5))\npyplot.title(\"Feasible (FNR, FPR) combinations\")\npyplot.xlim(0, 1)\npyplot.xlabel(\"False Negative Rate\")\npyplot.ylim(0, 1)\npyplot.ylabel(\"False Positive Rate\")\n\ngroup_labels = [\"White\", \"Black\", \"Other\"]\npalette = {\"White\": \"yellow\", \"Black\": \"black\", \"Other\": \"red\"}\n\nsns.scatterplot(x=FNR_groups, y=FPR_groups, hue = group_labels, palette=palette)\nsns.lineplot(x=fnr_values, y = white_fpr_values, color=\"yellow\")\nsns.lineplot(x=fnr_values, y = black_fpr_values, color=\"black\")\nsns.lineplot(x=fnr_values, y = other_fpr_values, color=\"red\")\n\npyplot.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Auditing Bias\n\n\n\n\n\nUsing the PUMS dataset to predict employment and audit for racial bias.\n\n\n\n\n\nMar 12, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nAn introduction into automated decision making systems with machine learning and their societal impacts\n\n\n\n\n\nMar 5, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Blog Post\n\n\n\n\n\nUsing the Palmer Penguins dataset to make predictions on species of penguins\n\n\n\n\n\nFeb 20, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]