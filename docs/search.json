[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Cameron Hudson’s Machine Learning Blog",
    "section": "",
    "text": "A blog to explore machine learning!"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html",
    "href": "posts/Implementing Perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "perceptron.py"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#perceptron-algorithm-on-linearly-seperable-data",
    "href": "posts/Implementing Perceptron/index.html#perceptron-algorithm-on-linearly-seperable-data",
    "title": "Implementing Perceptron",
    "section": "Perceptron Algorithm on Linearly Seperable Data",
    "text": "Perceptron Algorithm on Linearly Seperable Data\nHere, we will define the code that usings the functions from percepton.py to demonstrate the perceptron algorithm’s behavior on linearly seperable data.\n\n#Written by Professor Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\n#Written by Professor Chodrow\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#perceptron-algorithm-on-non-linearly-seperable-data",
    "href": "posts/Implementing Perceptron/index.html#perceptron-algorithm-on-non-linearly-seperable-data",
    "title": "Implementing Perceptron",
    "section": "Perceptron Algorithm on Non-Linearly Seperable Data",
    "text": "Perceptron Algorithm on Non-Linearly Seperable Data\nIn non-linearly seperable data, the loss function will not reach 0. In our original code, because of the while loop while loss &lt; 0, we will have an infinitely running loop in the case of non-linearly seperable data. To still show that the perceptron algorithm does attempt to seperate non-linearly seperable data, we will modify the code to halt after 1000 iterations, regardless if loss ever reaches 0 or not.\nWe create a function similar to perceptron_data(), except we increase the noise in order to make a set of data that is not linearly seperable.\n\ndef not_seperable_data(n_points, noise, p_dims):\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = not_seperable_data(300, 0.5, 2)\nplot_perceptron_data(X,y,ax)\n\n\n\n\n\n\n\n\nThen, slightly modify the code we used previously to show how the loss function updates the hyperplane, except we change our original while loop to while iterations &lt; 1000, increment iterations by 1 after each loop, and modify the original code so that we see the percptron’s loss and hyperplane every 200 iterations.\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\niterations = 0\nn = X.size()[0]\n\nwhile iterations &lt;= 1000:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if iterations % 200 == 0:\n        plot_perceptron_data(X, y, ax)\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        # This highlights the point choosen in each iteration\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f} iterations = {iterations}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\n    iterations+=1\nplt.tight_layout()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[88], line 33\n     30 local_loss = p.loss(x_i, y_i).item()\n     32 if local_loss &gt; 0:\n---&gt; 33     opt.step(x_i, y_i)\n     34 # if a change was made, plot the old and new decision boundaries\n     35 # also add the new loss to loss_vec for plotting below\n     36 if iterations % 200 == 0:\n\nTypeError: PerceptronOptimizer.step() missing 2 required positional arguments: 'alpha' and 'k'"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#results",
    "href": "posts/Implementing Perceptron/index.html#results",
    "title": "Implementing Perceptron",
    "section": "Results",
    "text": "Results\nHere we see that though it could not reach 0% loss, because the data is not linearly seperable, it still did a fairly good job in finding a hyperplane that seperated the data in 1000 iterations."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#more-than-2-dimensions",
    "href": "posts/Implementing Perceptron/index.html#more-than-2-dimensions",
    "title": "Implementing Perceptron",
    "section": "More than 2 Dimensions",
    "text": "More than 2 Dimensions\nThe perceptron algorithm is able to generate hyperplanes that seperate linearly seperable data that is more than just two dimensions. Below is an example of data with 5 features which corresponds to five dimensions. In the proceeding code, we will use the perceptron_data() function used either, only now we will define 5 dimensions instead of 2.\n\nX, y = perceptron_data(300, 0.2, 5)"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#loss-evolution",
    "href": "posts/Implementing Perceptron/index.html#loss-evolution",
    "title": "Implementing Perceptron",
    "section": "Loss Evolution",
    "text": "Loss Evolution\nWe will then make a modified version of the code earlier to assess how the loss evolves through Perceptron’s iterations.\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\niter_loss = []\niterations = 0\nn = X.size()[0]\n\nwhile iterations &lt;= 1000:\n     i = torch.randint(n, size = (1,))\n     x_i = X[[i],:]\n     y_i = y[i]\n     opt.step(x_i, y_i)\n\n\n     if (iterations % 200 == 0):\n          iter_loss.append(p.loss(X, y).item())\n     iterations+=1\n\nprint(iter_loss)\n\n[0.4333333373069763, 0.0033333334140479565, 0.0033333334140479565, 0.0033333334140479565, 0.05999999865889549, 0.0]"
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#results-1",
    "href": "posts/Implementing Perceptron/index.html#results-1",
    "title": "Implementing Perceptron",
    "section": "Results",
    "text": "Results\nBased on the losses calcuated every 200 iterations, we can conclude from the final loss function of 0 that this set of data is indeed linearly seperable."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#perceptronoptimizer.loss-runtime-complexity",
    "href": "posts/Implementing Perceptron/index.html#perceptronoptimizer.loss-runtime-complexity",
    "title": "Implementing Perceptron",
    "section": "PerceptronOptimizer.loss() Runtime Complexity",
    "text": "PerceptronOptimizer.loss() Runtime Complexity\nThis function takes in two tensors X and y and returns a percentage. In order to do this we must call the score() function which computes the dot product of our current weight vector and all of the rows of X, corresponding to feature vectors. This score() function has us do a O(p) operation (where p is the number of features) n times (where n is the number of points or rows in X). So far our overall runtime complexity is O(n*p). After calculating the score, which multiples the each value in the score tensor to each value in the y_ tensor, evalutes whether this product is &gt; or &lt; than 0, converts the True and False values to 1’s and 0’s, sums these values, and divides it by the total number of score values. The rest of these operations listed are all linear, so our overall time complexity remains O(n*p)."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#perceptronoptimizer.grad-runtime-complexity",
    "href": "posts/Implementing Perceptron/index.html#perceptronoptimizer.grad-runtime-complexity",
    "title": "Implementing Perceptron",
    "section": "PerceptronOptimizer.grad() Runtime Complexity",
    "text": "PerceptronOptimizer.grad() Runtime Complexity\nThis function also calls the score() function, which we have previous disovered is an O(n*p) operation, and store the result in score_i. Afterwards, we perform operations on the result of score_i which we store in misclassified, and yet again perform operations on misclassified which which are returned as the function’s output. The rest of these operations are O(n), and thus our overall time complexity of this function O(n*p)."
  },
  {
    "objectID": "posts/Implementing Perceptron/index.html#overall-time-complexity",
    "href": "posts/Implementing Perceptron/index.html#overall-time-complexity",
    "title": "Implementing Perceptron",
    "section": "Overall Time Complexity",
    "text": "Overall Time Complexity\nThe big-O complexity of PerceptronOptimizer.step(), because its most taxing operation had a time complexity of O(n*p), is thus O(n*p)."
  },
  {
    "objectID": "posts/ Sparse Kernel Machines/index.html",
    "href": "posts/ Sparse Kernel Machines/index.html",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic_kernel import KernelLogisticRegression\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nlogistic_kernel.py"
  },
  {
    "objectID": "posts/ Sparse Kernel Machines/index.html#λ-is-large",
    "href": "posts/ Sparse Kernel Machines/index.html#λ-is-large",
    "title": "Sparse Kernel Machines",
    "section": "1: λ Is Large",
    "text": "1: λ Is Large\nThe λ value controls how strong l2 regularization is in this kernelized logistic regression model, which is a method of penalizing large weights. With a large enough λ value, weights that do not significantly lower the objective loss function are pushed to a value of 0. In the experiment below, I show how a large enough λ value can result in all weights being driven to 0 by l2 regularization expect for 1 point, which is outlined in black.\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 163, gamma = 1)\nKR.fit(X, y, m_epochs = 50000, lr = 0.01)\n\nix = (torch.abs(KR.a) &gt; 0.001).squeeze() \n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_, recompute_kernel = True)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n# ax.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")"
  },
  {
    "objectID": "posts/ Sparse Kernel Machines/index.html#changing-γ",
    "href": "posts/ Sparse Kernel Machines/index.html#changing-γ",
    "title": "Sparse Kernel Machines",
    "section": "2: Changing γ",
    "text": "2: Changing γ\nThe γ value within kernelized logistic regression essentially controls how wide of an influence each point has in the decision boundary, with the width decreasing as γ increases. As the experiment below demonstrates, the decision boundary is smoother when γ is low, as points far from each other have a larger influence on the decision function. Larger γ values however signal that only points close to each other have a strong influence on the overall shape of the decision boundary, making the model more prone to fitting to noise and having a decision boundary that perfectly wraps around the training data.\n\nfig, ax = plt.subplots(1, 4, figsize=(16, 4))\nplt.suptitle(\"Decision Boundary with increasing γ values\", fontsize=14)\ngamma = [0.1, 1, 10, 100]\n\nfor i in range(4):\n    KR = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=gamma[i])\n    KR.fit(X, y, m_epochs=50000, lr=0.01)\n\n    ix = (torch.abs(KR.a) &gt; 0.001).squeeze()\n\n    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n    X_ = torch.stack((X1.ravel(), X2.ravel()), dim=1)\n\n    preds = KR.score(X_, recompute_kernel=True).reshape(X1.shape)\n\n    ax[i].contourf(X1, X2, preds, origin=\"lower\", cmap=\"BrBG\",\n                   vmin=2 * preds.min() - preds.max(), vmax=2 * preds.max() - preds.min())\n    plot_classification_data(X, y, ax[i])\n    ax[i].scatter(X[ix, 0], X[ix, 1], facecolors=\"none\", edgecolors=\"black\")\n    ax[i].set_title(f\"gamma = {gamma[i]}\")\n\nfig.tight_layout(rect=[0, 0, 1, 0.95])"
  },
  {
    "objectID": "posts/ Sparse Kernel Machines/index.html#kernel-methods-on-nonlinear-patterns",
    "href": "posts/ Sparse Kernel Machines/index.html#kernel-methods-on-nonlinear-patterns",
    "title": "Sparse Kernel Machines",
    "section": "3: Kernel Methods on Nonlinear Patterns",
    "text": "3: Kernel Methods on Nonlinear Patterns\nIn order to demonstrate how kernelized logistic regression can fit to nonlinear data, in the experiment below we will generate a 2d set of nonlinear data with sklearn’s make_moons, using the plot_decision_regions function to show the decision boundary that is creating by this kernelized logistic regression implementation.\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom mlxtend.plotting import plot_decision_regions\n\nX_np, y_np = make_moons(n_samples=200, noise=0.2, random_state=42)\n\n# Step 2: Convert to PyTorch tensors\nX = torch.tensor(X_np, dtype=torch.float32)\ny = torch.tensor(y_np, dtype=torch.float32)\n\n# Step 3: Fit the model\nKR = KernelLogisticRegression(rbf_kernel, lam=0.001, gamma=1)\nKR.fit(X, y, m_epochs=500000, lr=0.001)\n\n\nfig, ax = plt.subplots(1, 1)\nplot_decision_regions(X_np, y_np, clf = KR, ax = ax)"
  },
  {
    "objectID": "posts/Automated Decision Systems/index.html",
    "href": "posts/Automated Decision Systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Abstract\nToday, we are going to build a automated decision system that will be demonstrated using a dataset of borrowers from a bank. The objective of this decising making model is to use the training data to predicting whether a not a future applicant is likely to default a loan or repay it in full. We will use this model not only to demonstrate the creation of a decision making model, but also to assist a larger discussion on its societal impacts.\n\n\nGrabbing and Observing the Data\nFirst we import the dataset that includes previous borrowers:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\nWith df.head() we can see the features that are associated with each borrower:\n\ndf.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\n\nBase Rates\nTo demonstrate that our machine learning model has indeed derived a pattern from the training data, we will try to achieve an accuracy above the base rate. The base rate is the accuracy that we can achieve if we predict one outcome for every possible decision.\n\n1-df[\"loan_status\"].mean()\n\nnp.float64(0.7824201964395334)\n\n\nin this case, loan_status equals 1 if the borrower did default on their loan, and 0 if they repaid it in full. df[\"loan_status\"].mean gets the percentage of borrowers that defaulted, and subtracting by 1 gives us the perctange of borrows that repaid the loan. If we always predict the applicant will repay the loan, then we will get correct roughly 78% of the time. In order to demonstrate learning by the model, we must aim for an accuracy above 78%.\n\n\nVisualizing the Data\nWe want to make informed decisions on the data, which requires that we search for patterns within that data. One question we may ask is: is there a observable pattern of loan intent with respect to age, length of employment, or homeownership status?\nTo discover this, we will make visualizations of our dataset to prove or dispprove the relation between age, length of employment, or homeownership status and loan intent. We will make use of the LabelEncoder in order to turn our values of loan intent (venture, education, home improvement) into values that will be easier to work with.\n\n\nSummary Table\n\nsummary_table = df.groupby(\"person_home_ownership\").agg({\n    \"loan_amnt\": \"mean\",\n    \"loan_int_rate\": \"mean\",\n    \"loan_status\": \"mean\" \n})\nsummary_table\n\n\n\n\n\n\n\n\nloan_amnt\nloan_int_rate\nloan_status\n\n\nperson_home_ownership\n\n\n\n\n\n\n\nMORTGAGE\n10562.137462\n10.491245\n0.125058\n\n\nOTHER\n11235.795455\n12.059221\n0.306818\n\n\nOWN\n8978.912626\n10.850169\n0.080653\n\n\nRENT\n8843.507973\n11.448571\n0.313971\n\n\n\n\n\n\n\n\n\nDiscussion\nThis summary table displays the mean loan amount, loan interest rate, and the rate of defaulting the loan based on the person’s home ownership status. We can see that the loan interest rate is about the same for every home ownership type, around 10-12%, as well as the loan amount, however the default rate differs quite dramatically between groups. Those who own their homes statistically almost never default, however those who are renting or have their home ownership status as “OTHER”, have a default rate around 30%. People in the dataset with their home ownership status as “mortgage” have a default rate lower at 12%.\n\n\nScatterplot 1 using loan_amnt and loan_int_rate\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(figsize = (8, 3.5))\n\np2 = sns.scatterplot(df, x = \"loan_amnt\", y = \"loan_int_rate\", ax = ax, hue = \"loan_status\", palette=\"Set2\")\n\n\n\n\n\n\n\n\n\n\nPatterns in the Data\nWe can discern from this visualization that in this dataset, whether a borrower defaults or not is not very dependent on the loan amount, but their interest rate, where borrowers with higher interest rates, usually greater than 13%, are more likely to default than borrowers with interest rates below 13%.\n\n\nScatter plot 2 using cb_person_default_on_file\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig, ax = plt.subplots(figsize = (8, 3.5))\n\np2 = sns.scatterplot(df, x = \"loan_amnt\", y = \"loan_int_rate\", ax = ax, hue = \"cb_person_default_on_file\", palette=\"Set2\")\n\n\n\n\n\n\n\n\n\n\nPatterns in the Data\nHere we see a similar split between the data, whereby borrowers who have previously defaulted on their loans have higher interest rates then those who payed back their previous loan in full. As we have seen from the last visualization that interest rates are corellated with the rate of defaulting the loan, we may want to consider if they have previously defaulted in our prediction.\n\n\nBuilding the Prediction Model\nIn order to build our model, we are first going to have to prepare our data to support accurate prediction.\n\n\nData Processing\nOur first step to processing the data to be trained is to drop unnessecary rows and columns. In this instance, we must drop all rows that do not contain a value for one of their columns, as well as our target variable loan status and loan grade. We use X = pd.get_dummies(X) to hot encode our categorical features such as loan_intent, person_home_ownership, and cb_person_default_on_file, store our target variable in the variable y, and use sklearn’s train_test_split to split the data into training and testing data for cross validation.\n\nfrom sklearn.model_selection import train_test_split\n\ndef prepare_data(df):    \n    df = df.dropna()\n    X = df.drop([\"loan_grade\", \"loan_status\"], axis=1)  \n    X = pd.get_dummies(X)\n    y = df[\"loan_status\"]\n    return X, y\nX, y = prepare_data(df)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_emp_length\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_cred_hist_length\nperson_home_ownership_MORTGAGE\nperson_home_ownership_OTHER\nperson_home_ownership_OWN\nperson_home_ownership_RENT\nloan_intent_DEBTCONSOLIDATION\nloan_intent_EDUCATION\nloan_intent_HOMEIMPROVEMENT\nloan_intent_MEDICAL\nloan_intent_PERSONAL\nloan_intent_VENTURE\ncb_person_default_on_file_N\ncb_person_default_on_file_Y\n\n\n\n\n21383\n35\n50532\n8.0\n5000\n13.57\n0.10\n8\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n2116\n25\n60000\n5.0\n1200\n13.85\n0.02\n2\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\n\n\n5631\n30\n55000\n1.0\n6500\n10.99\n0.12\n7\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n20675\n30\n34046\n2.0\n12000\n14.96\n0.35\n6\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n9582\n23\n70522\n7.0\n9325\n10.91\n0.13\n3\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13577\n32\n52000\n0.0\n2500\n8.88\n0.05\n10\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n24532\n36\n60000\n7.0\n17050\n12.69\n0.28\n14\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6125\n22\n27264\n4.0\n5000\n13.11\n0.18\n3\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n974\n35\n50000\n0.0\n5600\n12.53\n0.11\n7\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n17904\n28\n47000\n5.0\n9000\n12.42\n0.19\n8\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n18325 rows × 19 columns\n\n\n\n\n\nPicking the Features and Fitting the Model\nWe will be using sklearn’s SelectKBest with the scoring function mutual_info_classif to get the most predictive features, limiting the number of features to 6 to lower the amount of iterations the machine learning model must compute.\n\nfrom sklearn.feature_selection import SelectKBest, mutual_info_classif\nselector = SelectKBest(score_func=mutual_info_classif, k=6)\nX_selected = selector.fit_transform(X_train, y_train)\n\nfeatures = X.columns[selector.get_support()]\n\nprint(features)\n\nIndex(['person_income', 'loan_int_rate', 'loan_percent_income',\n       'person_home_ownership_MORTGAGE', 'person_home_ownership_RENT',\n       'cb_person_default_on_file_N'],\n      dtype='object')\n\n\n\n\nBuilding the Model\nNow that we have obtained our features, we create a logistic regression model and train it on X_train, using only our selected models with X_train[features]. After fitting the model, we can produce a score that will show our models predictive accuracy.\n\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(max_iter=50000)\nLR.fit(X_train[features], y_train)\nLR.score(X_train[features], y_train)\n\n0.8474761255115962\n\n\n\n\nResults\nWe see through our score that we got a higher predictive accuracy than our base rate of 78%, showing that are model indeed learned patterns through our training data and selected features.\n\nLR.score(X_test[features], y_test)\n\n0.8384984722828459\n\n\nWe get similar results on new data, showing that are model was able to create generalizations that can be extrapolated to predict unseen data.\n\n\nOur Goal\nOur goal now is to find a threshold that will provide a binary answer to the question of whether our not the bank should give a loan to a potential appliant and to compute the overall profit gained or lost by the bank.\n\n\nApproach\nThe apporach we will be taking is to first calculate a score to give each applicant s, with that score being the likelihood of whether or not an applicant will default on their loan or not. If this score is higher than our choosen threshold, the model will predict that the applicant will default on the loan and the bank will not lend to the applicant. If this score is lower or equal to our choosen threshold, the bank will lend\n\n\nCalculations\nIn order to evaluate the total profit that the bank gains or losses given a particular threshold value, we will assume for convience that if the loan is repaid in full, the profit for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt and if the borrower defaults on the loan, the “profit” for the bank is equal to loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt. If the bank doesn’t give out a loan, their profit is 0 for that particular borrower.\n\n\nWeights\nOur weights, stored in the variable in LR.coef where LR is our logistic regression model shows an array with all of the weights for our chosen features respectively. This is also known as our weight vector, which we can then use to find the threshold in order for our model to produce a yes or no answer.\n\nweight_vector = LR.coef_\nweight_vector\n\narray([[-1.08054136e-05,  2.91142437e-01,  7.80074168e+00,\n         1.18823376e+00,  1.98583970e+00, -1.00706707e-01]])\n\n\n\n\nFinding a Threshold\nIn order to create an automated system that finds the threshold that maximizes the total profit, we are going to split this task up into two functions: individual_profit, and find_threshold.\n\n\nIndividual Profit Function\nThis function takes the loan amount and loan interest rate of a borrower and computes based on their score and the threshold the profit that the bank will make dealing with this applicant. If the score is greater than our chosen threshold, the bank’s profit will be 0, because the bank will not lend to the potiental borrower. In the case where the model predicts the applicant will repay the loan, we then are faced with two more options. If the applicant actaully did repay the loan, that the profit is the repaid_profit value discussed previously. If the bank lends to an applicant but they default, then the bank in this case will lose profit, shown by the value of default_profit.\n\nimport numpy as np\ndef individual_profit(loan_amnt, loan_int_rate, score, threshold, loan_status):\n        loan_int_rate = loan_int_rate / 100\n        repaid_profit = loan_amnt*(1 + 0.25*loan_int_rate)**10 - loan_amnt\n        defaulted_profit = loan_amnt*(1 + 0.25*loan_int_rate)**3 - 1.7*loan_amnt\n        return np.where(score &lt;= threshold, \n                        np.where(loan_status == 1, defaulted_profit, repaid_profit), \n                        0)\n\n\n\nFind Threshold Function\nThis function uses np.linspace(0,1,1000) to enumerate through 1000 threshold values evenly spaced between 0 and 1, and notes the total profit gained from that selected threshold with the value of total_profit using the np.sum function. If that profit is greater than our value for max_profit, then we update max_profit and best_threshold to reflect the threshold value that gives the bank the maximum profit.\n\nimport numpy as np\ndef find_threshold(df, y, features, model):\n    max_profit = float(\"-inf\")\n    best_threshold = 0\n    scores = model.predict_proba(df[features])[:, 1]\n    thresholds = np.linspace(0, 1, 1000)\n    for threshold in thresholds:\n        total_profit = np.sum(individual_profit(df[\"loan_amnt\"], df[\"loan_int_rate\"], scores, threshold, y))\n        if total_profit &gt; max_profit:\n            max_profit = total_profit\n            best_threshold = threshold\n\n    return best_threshold, max_profit\n    \n\n\n\nResults\nOur resulting function reports that the best threshold value is 0.44 which will predict a profit of $25,728,462.\n\nbest_threshold, max_profit = find_threshold(X_train, y_train.values, features, LR)\n(best_threshold, max_profit)\n\n(np.float64(0.44544544544544545), np.float64(25728462.290848892))\n\n\n\n\nTraining Threshold to Profit Relationship\nThrough this graph, we can also demonstrate how the change in threshold value increases and decreases the profit the bank makes per loan borrower, showing that indeed a threshold value of approximately 0.4 results in the most profit for the bank.\n\nscores = LR.predict_proba(X_train[features])[:, 1]\n\n# Store thresholds and profits per borrower\nthresholds = np.linspace(0, 1, 1000)\nprofits_per_borrower = []\nbest_profit_per_borrower = max_profit / len(X_train)\n\nfor threshold in thresholds:\n    total_profit = np.sum(individual_profit(\n        X_train[\"loan_amnt\"],\n        X_train[\"loan_int_rate\"],\n        scores,\n        threshold,\n        y_train\n    ))\n    profit_per_borrower = total_profit / len(X_train)\n    profits_per_borrower.append(profit_per_borrower)\n\n# Plotting\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\nax.plot(thresholds, profits_per_borrower)\nax.plot(best_threshold, best_profit_per_borrower, 'ko')\nax.set_ylabel(\"Profit per Prospective Borrower (Training Set)\")\nax.set_xlabel(\"Threshold\")\nax.set_title(\"Profit per Borrower vs. Threshold\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nModel Evaluation: Bank’s Perspective\nTo sum up our creation of a predictive decision making model, we are going to test it on the data stored in df_test.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\nX_df_test, y_df_test = prepare_data(df_test)\nLR.fit(X_df_test[features], y_df_test)\n# weight_vecator = LR.coef_\nthreshold, profit = find_threshold(X_df_test, y_df_test, features, LR)\nthreshold, profit\n\n(np.float64(0.4194194194194194), np.float64(7703261.7572102975))\n\n\n\n\nTesting Results\nRunning our automated decision making system on the testing data gives a similar optimal threshold as the training data, proving that for this dataset a threshold value of around 0.4 results in the most profit for the bank.\n\n\nModel Evaluation: Borrower’s Perspective\nLets now evaluate how the model treats individual borrowers based on their age, loan intent and income. the main questions we want to ask are those of discrimination: Is it more difficult for people in certain age groups to access credit? Is it more difficult for people to get loans in order to pay for medical expenses? How does a person’s income level impact the ease with which they can access credit?\n\n\nDifferences in Age Groups Access To Credit\nTo answer this question we can make a summary table. We use the pd.cut function to make continous variables into categorical variables.\n\nimport pandas as pd\nimport numpy as np\n\n# Create age groups\nage_bins = [18, 25, 35, 50, 65, 100]\nage_labels = [\"&lt;25\", \"25-35\", \"35-50\", \"50-65\", \"&gt;65\"]\ndf_test[\"age_group\"] = pd.cut(df_test[\"person_age\"], bins=age_bins, labels=age_labels)\n\n# Prepare test data (ensure one-hot encoding matches training data)\nX_df_test, y_df_test = prepare_data(df_test)\ndf_test_filtered = df_test.loc[X_df_test.index].copy()\n\n# Predict scores using prepared data\nscores = LR.predict_proba(X_df_test[features])[:, 1]\n\n# Approval decision\ndf_test_filtered[\"approved\"] = scores &gt; threshold\n\n# Compute acceptance rates by age group\nage_acceptance_rates = df_test_filtered.groupby(\"age_group\")[\"approved\"].mean()\ndisplay(age_acceptance_rates)\n\n/var/folders/pv/gfxr_25s0q9gt_2k6h_bbgmc0000gn/T/ipykernel_98691/2402408799.py:20: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  age_acceptance_rates = df_test_filtered.groupby(\"age_group\")[\"approved\"].mean()\n\n\nage_group\n&lt;25      0.209654\n25-35    0.175219\n35-50    0.142857\n50-65    0.268293\n&gt;65      0.000000\nName: approved, dtype: float64\n\n\n\n\nResults\nWhat we can see from the function’s output is that people higher in age (likely correlated with higher income) are more likely of being lent a loan.\n\n\nDifficuly Getting Loans for Medical Expenses\nTo answer this question, we will use the pd.groupby() function to group all of the applicants in df_test into the two groups loan_intent_acceptance which contains only the applicants that got their loan request approved and gets the percentage of those applicants whos loan intent was one of the following loan_intent values, and loan_intent_default which the same as loan_intent_acceptance except that it contains the true rate of default for that group.\n\nloan_intent_acceptance = df_test_filtered.groupby([\"loan_intent\"])[\"approved\"].mean()\n\n\nloan_intent_default = df_test_filtered.groupby([\"loan_intent\"])[\"loan_status\"].mean()  \n\nloan_analysis = pd.DataFrame({\n    \"Acceptance Rate\": loan_intent_acceptance,\n    \"Default Rate\": loan_intent_default\n})\ndisplay(loan_analysis)\n\n\n\n\n\n\n\n\nAcceptance Rate\nDefault Rate\n\n\nloan_intent\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.200221\n0.287611\n\n\nEDUCATION\n0.176871\n0.167517\n\n\nHOMEIMPROVEMENT\n0.154221\n0.250000\n\n\nMEDICAL\n0.211556\n0.284250\n\n\nPERSONAL\n0.201403\n0.220441\n\n\nVENTURE\n0.176349\n0.146266\n\n\n\n\n\n\n\n\n\nIncome Level vs Access to Credit\nTo answer this question we take the same approach we took in tacking difference in loan availibility based on age groups to answer the question based on income groups.\n\n# Add income group to the filtered DataFrame\nincome_bins = [0, 30000, 60000, 100000, 500000]\nincome_labels = [\"&lt;30K\", \"30K-60K\", \"60K-100K\", \"&gt;100K\"]\n\ndf_test_filtered[\"income_group\"] = pd.cut(df_test_filtered[\"person_income\"], bins=income_bins, labels=income_labels)\n\n# Group by income group and compute acceptance rate\nincome_acceptance_rates = df_test_filtered.groupby(\"income_group\")[\"approved\"].mean()\ndisplay(income_acceptance_rates)\n\n/var/folders/pv/gfxr_25s0q9gt_2k6h_bbgmc0000gn/T/ipykernel_98691/1191128577.py:8: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  income_acceptance_rates = df_test_filtered.groupby(\"income_group\")[\"approved\"].mean()\n\n\nincome_group\n&lt;30K        0.383007\n30K-60K     0.234381\n60K-100K    0.105170\n&gt;100K       0.028302\nName: approved, dtype: float64\n\n\n\n\nResults\nHere, we see that applicants who have a higher income have a much higher chance of getting a loan request accepted then applicants with lower incomes.\n\n\nOverall Reflection\nHere we highlight notable problems with a decision making algorithm deciding whether or not applicants deserve loans or not. As the model, weights, and threshold were decided with the intention of garnering the bank with the maximum amount of profit, there were considerable issues regarding how the model treated individual applicants. My findings note that the need for the loan (due to lower age, income, or important reasons such as a medical expenses) is inversely related to the availability of the loan under this decision making system."
  },
  {
    "objectID": "posts/Limits of the Quantitative Approach to Bias and Fairness/index.html",
    "href": "posts/Limits of the Quantitative Approach to Bias and Fairness/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "This post explores the benefits and drawbacks of using quantitative methods to measure and detect discrimination. Quantitative approaches rely on mathematical definitions, machine learning algorithm auditing, and statistical tests to identify biases. However, Narayanan (2022) argues these methods often fail to capture the complexity of discrimination, leading to misleading conclusions and reinforcing existing inequalities. This post will examine Narayanan’s critique while also acknowledging cases where quantitative analysis has effectively exposed bias."
  },
  {
    "objectID": "posts/Limits of the Quantitative Approach to Bias and Fairness/index.html#limitations-of-quantitative-methods",
    "href": "posts/Limits of the Quantitative Approach to Bias and Fairness/index.html#limitations-of-quantitative-methods",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Limitations of Quantitative Methods",
    "text": "Limitations of Quantitative Methods\nOther scholars including Narayanan report the various limitations of quantitative methods in detecting discrimination. Narayanan, in his speech, specifically refers to ProPublica as being the cause of his disillusionment with the usefulness of quantitative methods. Narayanan expounds on his viewpoint referring to the developers of the COMPAS RPI in the quote, “If the developers of risk prediction algorithms redesigned them to equalize the rates of falsely flagging someone as high risk, between Black defendants and white defendants, that doesn’t solve the problem. It remains profoundly unjust to deny someone their freedom based on a prediction that they might commit another crime or a prediction that they might not appear in court for their arraignment or their trial”(Narayanan (2022)). The dilemma which Narayanan describes can be explained through description of the differences between the “broad view” and the “middle view” in Barocas, Hardt, and Narayanan (2023). What Narayanan describes equalizing error rates would align with is be the “middle view”: the equalization of error rates in RPI’s and discriminatory systems refers to an enforcement of fairness at the decision-making level. What Narayanan believes is right, however, is more in line with the “broad view”. The broad view states that fairness should be addressed at the systemic level, and that the racial biases of RPI’s are a symptom of a larger problem. Through the example of ProPublica’s COMPAS audit, Narayanan demonstrates that even when quantitative methods can reveal injustice, they often do not provide the means or solutions to the root cause of those injustices. Within chouldechova2017fair,ProPublica’s COMPAS audit is also criticized on its choice of fairness criteria, demonstrated in the quote, “Flores et al. [6] argue that the correct approach for assessing RPI bias is instead to check for calibration, a fairness criterion that they show COMPAS satisfies. Northpointe in their response[7] argue for a still different approach that checks for a fairness criterion termed predictive parity, which they demonstrate COMPAS also satisfies” (chouldechova2017fair). Chouldechova continues in his paper to prove algebraically that when prevalence differs among groups, the fairness criterions error rate parity and predictive parity can be satisfied simultaneously. The mathematical proof and reported push back of ProPublica’s audit in Chouldechova’s paper illustrate how quantitative methods can be insufficient in detecting real group bias as decision making systems can fulfill certain fairness criterion and not others. Other scholars report that the limits of quantiative methods lie in their reliance on incomplete or missing data. D’ignazio and Klein (2023) highlight how Black women in the United States suffer disproportionately high maternal mortality rates. Yet, when researchers sought to investigate this disparity, they found a shocking reality—no reliable datasets existed to confirm or study the problem. As D’Ignazio and Klein note, “Nobody was counting.” A 2014 United Nations report described maternal mortality data collection in the United States as “particularly weak.” This example underscores a critical issue: quantitative methods become ineffective when the necessary data does not exist. The lack of relevant data often stems from the demographics of those responsible for collecting and analyzing it. D’Ignazio and Klein highlight that, as of 2018, only 26% of professionals in “computer and mathematical operations” were women, and only 12% of those were Black or Latinx women, despite these groups comprising 22.5% of the U.S. population.\nThis demographic imbalance creates a privilege hazard, where those in positions of power and influence—who are not personally affected by systemic discrimination—fail to recognize its existence, leading to the invisibility of marginalized experiences in data collection. Other scholars further critique the assumption that fairness can be adequately measured through mathematical models. The fairness of a machine learning system extends beyond technical accuracy and error rate parity; it also involves broader social and epistemological concerns. As one study points out, “The fairness of a data science project extends far beyond the technical properties of a given model and includes normative and epistemological issues that arise during processes of problem formulation, data collection, and real-world application.” In other words, quantitative metrics alone cannot account for the deeper structural inequalities that exist before any algorithm is even built. Hardt and Recht (2022) encapsulate this critique by arguing that discriminatory design does not require intentional malice. As he explains, “One need not harbor any racial animus to exercise racism… rather, when the default settings have been stipulated, simply doing one’s job…is enough to ensure the consistency of white domination over time.” This highlights how data-driven decision-making often reinforces existing social hierarchies, even in the absence of explicit bias.\nThis idea is further emphasized by Eubanks (2018), who discusses in her article how automated systems are designed to perpetuate inequality and increase the ethical distance between human decision-makers, thereby justifying systemic bias. The article introduces the term “rational discrimination,” which serves as the justification and backbone of biased algorithms. Eubanks states, “Rational discrimination does not require class or racial hatred, or even unconscious bias, to operate. It requires only ignoring bias that already exists.” This aligns with the argument that data collection itself is a central issue in fairness for machine learning algorithms—one that cannot be addressed merely by measuring algorithmic outcomes across different groups. If the data does not accurately reflect the reality of discrimination or contains inherent biases, then no amount of technical auditing can uncover the deeper inequities that were embedded long before the algorithm was even created. This notion of “rational discrimination” aligns with Narayanan’s critique of quantitative methods, which he argues often serve to justify the status quo rather than challenge it.\nIn a fundamentally discriminatory system, rational discrimination operates within what Barocas, Hardt, and Narayanan (2023) refer to as the “narrow view.” The narrow view asserts that people who are similar with respect to a task should be given similar opportunities and rewards, evaluating fairness purely at the level of the individual without considering how systemic discrimination shapes opportunities in the first place. This framework falsely presents itself as “meritorious” and “fair,” despite statistical realities suggesting otherwise—such as Narayanan’s observation that only 1% of Fortune 500 CEOs are Black. What rational discrimination fails to consider are the perspectives captured in the “middle view” and “broad view,” which acknowledge historical oppression and advocate for interventions that challenge systemic inequality. These perspectives call for active efforts to remedy disparities rather than passively accepting them as natural outcomes of a supposedly neutral process. However, such interventions are often absent from quantitative methods, as these issues extend beyond what numerical evaluations alone can address. As Narayanan explains, quantitative analyses frequently produce superficially plausible explanations that discourage further investigation or action. This is evident in his critique of the Fortune 500 CEO statistic: “As a quantitative scholar, you’re not allowed to conclude from this that there is discrimination in Fortune 500 companies. You’re supposed to be open to all possibilities. Like maybe Black people just aren’t that interested in becoming CEOs (Narayanan (2022)).” This is just one example of how quantitative methods, when used in isolation, fail to meaningfully challenge discrimination. By allowing for convenient yet flawed interpretations, these methods risk reinforcing existing disparities rather than prompting meaningful change."
  },
  {
    "objectID": "posts/Penguin Blog Post/index.html",
    "href": "posts/Penguin Blog Post/index.html",
    "title": "Palmer Penguins Blog Post",
    "section": "",
    "text": "Abstract\nWelcome! Today, I am going to demonstrate how to train a Logistic Regression model in order to make predictions on species of penguins using the Palmer Penguins dataset. We will cover how to find visualize and select good qualitative and quantiative predictors, bruteforce feature selection, how to plot decision regions, confusion matrices in attempt to show a comprehensive guide of the process of making a predictor model with a dataset.\n\n\nAcquring the Data\nWe are first going to import two popular data science python libraries, pandas and numpy. After this, we are going to get our Palmers Penguins dataset from the url listed below, and use pandas to convert the csv file into a DataFrame which will be stored in the variable train.\n\nimport pandas as pd\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nLet’s see 5 of the pieces of data we are working with using the head() method.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nData Preperation\nWe are now going to prepare the data. Here we drop columns that are likely unhelpful in making a prediction on the species of penguins, specifying axis = 1 to delete the column instead of the index. We then remove all rows were the value of df[“Sex”] is undefined or “.”. We use the method dropna() to drop all the rows that contain missing values. We then use the method fit() from the LabelEncoder object to pick numerical values from qualitative values. For example, our penguin species are “Gentoo”, “Chinstrap”, and “Adelie” which could be encoded as 0, 1, and 2. Now that we have encoded the values of the “Species” column, we create a column that uses that encoding with le.transform, and we want to use the encoded values on the “Species” column, so we will do le.transform(df[“Species”]) and store this new column in y. We will then drop the original Species column as we will not need it for our prediction. Finally, we do pd.get_dummies(df) which will convert all categorical variables that only have 2 unique values to a 0 or a 1. With this, we have prepared are data and are ready to use it to make predictions on species of Penguins.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nfor i, c in enumerate(le.classes_):\n    print(f\"Class number {i} represents {c} penguins.\")\n\nX_train, y_train = prepare_data(train)\n\nClass number 0 represents Adelie Penguin (Pygoscelis adeliae) penguins.\nClass number 1 represents Chinstrap penguin (Pygoscelis antarctica) penguins.\nClass number 2 represents Gentoo penguin (Pygoscelis papua) penguins.\n\n\n\n\nLabelEncoder\nHere, we also enumerated through the classes made by the LabelEncoder and used a fstring to cleanly show which number encoding referes to which penguin.\n\n\nData Preperation Results\nNow we can see that we have removed unnessecary columns, dropped rows with empty values, and changed qualitative columns to columns of True or False (1 or 0) values:\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nData Visualizations\nHere we are going to visualize the relationship between some quantative data to get a feel for what features might be useful for our predictor model.\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nle.classes_ = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nspecies_names = [le.classes_[i] for i in y_train]\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(X_train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(X_train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", ax = ax[1], hue = species_names, palette=\"Set2\")\n\n\n\n\n\n\n\n\nHere we can see that the relationship between Flipper Length and Body Mass is good for distinguishing between Gentoo penguins and the other two species (data shows that Gentoo penugins on average have a greater body mass and flipper length than the two other species), however it is not so good for dintinguishing between the Adelie and Chinstrap penguin species.\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[1], hue = species_names, palette=\"Set2\")\n\n\n\n\n\n\n\n\nWe can see here that utilizing the culmen length and the flipper length gives rather distinct groups where each species of penguins resides. While some points overlap, showing that these variables don’t 100% identify a penguin’s species, it is certainly better at identifying chinstrap and adelie penguins from the last set of variables.\n\n\nSummary Tables and Aggregate Values\nBeyond plotting which is good for visualizing quantiative features and there usefulness, we can use aggregate values to gain more information on quantiative features such as sex, island, and clutch completion.\n\ntrain.groupby(\"Clutch Completion\").size() / X_train.shape[0]\n\nClutch Completion\nNo     0.117188\nYes    0.957031\ndtype: float64\n\n\n\ntrain.groupby(\"Sex\").size() / X_train.shape[0]\n\nSex\n.         0.003906\nFEMALE    0.519531\nMALE      0.511719\ndtype: float64\n\n\n\ntrain.groupby(\"Island\").size() / X_train.shape[0]\n\nIsland\nBiscoe       0.511719\nDream        0.398438\nTorgersen    0.164062\ndtype: float64\n\n\n\n\nSummary Table Findings\nHere we can see the percentage of all the penguins split between the various qualitative features (Sex, Island, and Clutch Completion). With this we can see that clutch completion will probably not help us determine the species of a particular penguin as for 95% of the penguins in the dataset Clutch_Completion is true. On the other hand, it seems that half of the penguins are male and the other half female, and that a penguin’s location on one of the three islands is also quite varied, making these qualiative features more useful for predictions than cluch completion.\n\n\nFeature Selection\nOur current task now is to find three of these features listed where 2 are quantiative and 1 are quantitative that will predict with high accuracy the species of Penguin (either Gentoo, Chinstrap, or Adelie).\n\nprint(X_train.columns)\n\nIndex(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)',\n       'Island_Biscoe', 'Island_Dream', 'Island_Torgersen',\n       'Stage_Adult, 1 Egg Stage', 'Clutch Completion_No',\n       'Clutch Completion_Yes', 'Sex_FEMALE', 'Sex_MALE'],\n      dtype='object')\n\n\nBecause we have a relatively small amount of features, we are going to train a logistic regression model on every distinct combination of 2 quantitative feautures and 1 qualitative feature, and keep track in the variable max_score what combination of features (stored in best_features) achieves the highest acurracy.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\ntest_cols = X_train\nbest_features = ['None']\nmax_score = 0\n\nLR = LogisticRegression(max_iter=20000)\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    test_cols = X_train[cols]\n    m = LR.fit(test_cols, y_train)\n    score = LR.score(test_cols, y_train)\n    if score &gt; max_score:\n      max_score = score\n      best_features = cols\nbest_features = best_features[2:] + best_features[0:2]\nm = LR.fit(X_train[best_features], y_train)\nprint(max_score, best_features)\n\n0.99609375 ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\nWith this, we have discovered that training a logistic regression model on the qualitative feature Sex (Male or Female) and the quantitative features Culmen Length (mm) and Culmen Depth (mm) gives the highest prediction accurracy among all the other 3 feature combinations with a accuracy of 99.5% on the training data.\n\n\nTesting the model on unseen data\nWe have achieved high accuracy using the training data, but can we achieve similar results using a different subset of data?\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_features], y_test)\n\n0.9852941176470589\n\n\nWe can see that the accuracy of our model on new data is only slightly worse at 98.5%.\n\n\nPlotting Decision Regions\nWe can more explicitly display how our model is deciding the species of penguins with our selected features with this plot_regions function.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nWe are first going to run this using the training data:\n\nplot_regions(LR, X_train[best_features], y_train)\n\n\n\n\n\n\n\n\nThen, we are going to run it through the testing data:\n\nplot_regions(LR, X_test[best_features], y_test)\n\n\n\n\n\n\n\n\nHere we can see that the distinct regions that model predicted each of the 3 species of penguins would be located at mostly correlates with the actual positions and species of the penguin data points, giving a more visual representation of our accuracy value of 99.6% on training data and 98.5% on testing data.\n\n\nConfusion Matrix\nUsing the confusion_matrix function from sklearn, we can create a matrix that shows the accurate and inaccurate predictions of our model.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[best_features],)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 10,  1],\n       [ 0,  0, 26]])\n\n\n\n\nConfusion Matrix Interpretation\nHere, on the diagonal of the matrix is the correct guesses, with each row corresponding to each possible label prediction (Chinstrap, Adelie, or Gentoo). We can make the output of our confusion matrix more appearent by returning to our LabelEncoder and looping through its contents:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 10 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 1 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\n\n\nConfusion Matrix Results and Findings\nWe can see that the only error the model made on the testing data was that it misidentified a Chinstrap penguin for a Gentoo penguin. This correlates with the plot_regions output using the training data, as the points that represent Chinstrap penguins were often close or in the region that predicts a penguin will be a Gentoo penguin.\n\n\nClosing\nWith this, we have predicted with high accuracy the species of penguins within the Palmer Penguins dataset. Through the process of feature selection and plotting data we found that the sex, culmen length and depth of penguins within the dataset are the best features to predict the particular species of that penguin. This exercise has taught me the general workflow of creating a predictor model with sklearn, numpy and pandas from start to finish. I also learned techniques of plotting data (scatterplots, desicion regions) what a confusion matrix is and how to interpret it, and the process of feature selection."
  },
  {
    "objectID": "posts/Newton's Method and Adam/index.html",
    "href": "posts/Newton's Method and Adam/index.html",
    "title": "Newtons’s Method and Adam",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logisticNewtonAdam import LogisticRegression, GradientDescentOptimizer, NewtonOptimizer, AdamOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Newton's Method and Adam/index.html#experiment-1-comparing-standard-graident-descent-and-newtons-method",
    "href": "posts/Newton's Method and Adam/index.html#experiment-1-comparing-standard-graident-descent-and-newtons-method",
    "title": "Newtons’s Method and Adam",
    "section": "Experiment 1: Comparing Standard Graident Descent and Newton’s Method",
    "text": "Experiment 1: Comparing Standard Graident Descent and Newton’s Method\nNow that we have a binary classification dataset, we can declare two Logistic Regression models, one that will use the standard gradient opitimzer and one that will use Newton’s Method optimization."
  },
  {
    "objectID": "posts/Newton's Method and Adam/index.html#experiment-2-faster-convergence-using-newtons-method",
    "href": "posts/Newton's Method and Adam/index.html#experiment-2-faster-convergence-using-newtons-method",
    "title": "Newtons’s Method and Adam",
    "section": "Experiment 2: Faster Convergence Using Newton’s Method",
    "text": "Experiment 2: Faster Convergence Using Newton’s Method\nThrough this experiment I will demonstrate that Newton’s Method in certain circumstances can have a much faster convergence than Standard Gradient Descent optimizaiton. The circumstance shown below is when the data has low noise and low number of points and dimensions, and gradient descent is using a learning rate of 0.1 while Newton’s Method is using a learning rate of 100. In this case, Newton’s Method is able to converge faster than standard gradient descent.\n\nLR_GD = LogisticRegression()\nopt_GD = GradientDescentOptimizer(LR_GD)\n\n#Logistic Regression model with Newton's Method optimizer \nLR_NM = LogisticRegression()\nopt_NM = NewtonOptimizer(LR_NM)\n# Track loss over iterations\nlosses_gd = []\nlosses_nm = []\n\n# Training loop\nn_steps = 100\nfor _ in range(n_steps):\n    # Gradient Descent\n    opt_GD.step(X, y, 0.1, 0, False, 0)\n    loss_gd = LR_GD.loss(X, y).item()\n    losses_gd.append(loss_gd)\n\n    # Newton's Method\n    opt_NM.step(X, y, alpha=100)\n    loss_nm = LR_NM.loss(X, y).item()\n    losses_nm.append(loss_nm)\n\n# Plot convergence\nplt.plot(losses_gd, label=\"Gradient Descent\", marker='o')\nplt.plot(losses_nm, label=\"Newton's Method\", marker='x')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk (Loss)\")\nplt.title(\"Convergence Comparison: GD vs Newton\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/tests/index.html",
    "href": "posts/tests/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/tests/index.html#math",
    "href": "posts/tests/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Double Descent/index.html",
    "href": "posts/Double Descent/index.html",
    "title": "Double Descent",
    "section": "",
    "text": "Abstract\nIn this blog post I will demonstrate the “double descent” phenomeon that occurs when the number of features is much greater than the number of points in a dataset. This phenomenon is part of the reason why deep learning outperforms traditional machine learning. By increasing the number of features, the MSE on testing data rises sharply before a certain threshold known as the interpolation threshold before the MSE values go back down, which can result in optimal model performance.\n\n\nPart 0: Why Closed-Form MSE does not work when number of parameters &gt; number of points\nThe reason that we cannot compute the closed-form of equation 1 if p &gt; n is because XTX is not invertible, so the operation (XTX)-1 would fail. This is because matrices that are not full-rank are not invertible. We know that XTX is not full rank because X is an nxp matrix, and the maximum possible rank of a matrix is limited by its smallest dimension (which in this case is n). The dimensions of XTX are pxp, however its rank is strictly less than or equal to n, which is less than the number of columns in the matrix p. That means that the matrix XTX rank cannot possibly equal its number of columns, making the matrix not full rank and not invertible.\n\n\nPart 1: Implementing overparameterized LinearRegression Model\nWe will implement a variation of the linear regression model that will instead use the Moore-Penrose ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍pseudoinverse of X to calculate the optimal weight vector value, as it can be computed when the number of parameters p is greater than the number of points n. We will use this model to demonstrate double descent.\n\nimport torch\n\nclass LinearModel:\n\n    def __init__(self):\n        self.w = None \n\n    def score(self, X):\n        \"\"\"\n        Compute the scores for each data point in the feature matrix X. \n        The formula for the ith entry of s is s[i] = &lt;self.w, x[i]&gt;. \n\n        If self.w currently has value None, then it is necessary to first initialize self.w to a random value. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            s torch.Tensor: vector of scores. s.size() = (n,)\n        \"\"\"\n        if self.w is None: \n            self.w = torch.rand(X.size()[1])\n        return X@self.w\n\n    def predict(self, X):\n        \"\"\"\n        Compute the predictions for each data point in the feature matrix X. The prediction for the ith data point is either 0 or 1. \n\n        ARGUMENTS: \n            X, torch.Tensor: the feature matrix. X.size() == (n, p), \n            where n is the number of data points and p is the \n            number of features. This implementation always assumes \n            that the final column of X is a constant column of 1s. \n\n        RETURNS: \n            y_hat, torch.Tensor: vector predictions in {0.0, 1.0}. y_hat.size() = (n,)\n        \"\"\"\n        scores = self.score(X)\n        return torch.where(scores &gt;= 0, torch.tensor(1.0), torch.tensor(0.0))\n\nclass MyLinearRegression(LinearModel):\n    def predict(self, X):\n        return self.score(X)\n    \n    def loss(self, X, y):\n        error = (y - self.score(X))\n        return torch.pow(error, 2).mean()\n        \n\nclass OverParameterizedLinearRegressionOptimizer:\n    def __init__(self, model):\n        self.model = model\n\n    def fit(self,X,y):\n        self.model.w = torch.linalg.pinv(X) @ y\n        return\n\n\n\nPart 2: Demonstration on Simple Data\nWe will now see how this overparamterized linear regression fits to random nonlinear data. Below is the data in question.\n\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\n\n\n\n\n\n\n\n\n\n\nHow will the model fit to nonlinear data?\nWith the class below, we apply a feature map to X in order to allow it to fit to nonlinear data instead of simply producing a linear line of best fit.\n\nimport torch\n\ndef sig(x): \n    return 1/(1+torch.exp(-x))\n\ndef square(x): \n    return x**2\n\nclass RandomFeatures:\n    \"\"\"\n    Random sigmoidal feature map. This feature map must be \"fit\" before use, like this: \n\n    phi = RandomFeatures(n_features = 10)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n\n    model.fit(X_train_phi, y_train)\n    model.score(X_test_phi, y_test)\n\n    It is important to fit the feature map once on the training set and zero times on the test set. \n    \"\"\"\n\n    def __init__(self, n_features, activation = sig):\n        self.n_features = n_features\n        self.u = None\n        self.b = None\n        self.activation = activation\n\n    def fit(self, X):\n        self.u = torch.randn((X.size()[1], self.n_features), dtype = torch.float64)\n        self.b = torch.rand((self.n_features), dtype = torch.float64) \n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\n\n\nPutting it All Together\nNow, we use our RandomFeatures class, which we can specify how many features to use, and then fit and transform our X feature matrix in order to have nonlinearity. Through plotting the predictions made by the model after fitting, we can see that the model does well capturing the trend of this nonlinear data.\n\nphi = RandomFeatures(n_features = 10)\nphi.fit(X)\nX_train_features = phi.transform(X)\n\nLR = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(LR)\nopt.fit(X_train_features, y)\n\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype = torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\ny_pred = LR.predict(X_train_features)\n\nplt.scatter(X, y, color='darkgrey', label='Data')\nplt.plot(X, y_pred, label=\"Predictions\")\nplt.title(\"Overparameterized Linear Regression\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPart C: Double Descent In Image Corruption Detection\nNow we will demonstrate overfitting and double descent with an image corruption machine learning task. Through the code shown below, we will import an image of flower using sklearn.datasets. Afterwards, the corrupt_image() function will randomly make patches in the image one solid color, simulating image corruption. We will then use this as our regression task of predicting how many corrupt patches an image contains.\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\n\n\nUtilizing Overparameterized Linear Regression with Feature Maps\nNow that we have created the image corruption regression task, we will then create our dataset, use sklearn’s train_test_split to split the data into training and testing, and train our overparameterized linear regression mode, making sure to trasform X_train and y_train with the phi_transform() method to apply the feature map and allow the model to fit nonlinear data.\n\nn_samples = 200\n\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype = torch.float64)\ny = torch.zeros(n_samples, dtype = torch.float64)\nfor i in range(n_samples): \n    X[i], y[i] = corrupted_image(flower, mean_patches = 100)\n\n\nfrom sklearn.model_selection import train_test_split\nX = X.reshape(n_samples, -1)\n# X.reshape(n_samples, -1).size()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n\nnum_iterations = 200\nmse_training, mse_testing = [], []\nbest_testing_error = [float('inf'), 0]\nfor i in range(200):\n    LR = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(LR)\n\n    phi = RandomFeatures(n_features = i, activation=square)\n    phi.fit(X_train)\n    X_train_phi = phi.transform(X_train)\n    X_test_phi = phi.transform(X_test)\n    opt.fit(X_train_phi, y_train)\n    \n    cur_mse_training = LR.loss(X_train_phi, y_train).item()\n    cur_mse_testing = LR.loss(X_test_phi, y_test).item()\n\n    if(cur_mse_testing &lt; best_testing_error[0]):\n        best_testing_error[0] = cur_mse_testing\n        best_testing_error[1] = i\n\n    mse_training.append(cur_mse_training)\n    mse_testing.append(cur_mse_testing)\n\n\n\nModel Proformance\nBelow is the graph plotting the change in the MSE as the number of features used in the model increasing for training and testing. As shown in the training plot, when the number of features used exceeds the number of data points, the model perfectly fits to the training data and sees very minimal improvement after this interpolation threshold. Within the testing data, on the otherhand, as the number of features used approaches and reaches the number of data points, MSE shoots up as the model is unable to generalize to new data. As the number of features continues to increase past this interpolation threshold, however, the MSE starts to decrease, and can even be lower than the MSE’s found before passing the interpolation threshold. Within this experiment, for example, the interpolation threshold is seen to be at 100 features, however the optimal testing error was found when the model used 195 features, demontsrating the power of the double descent phenomenon.\n\nprint(f\"Best testing mse: {best_testing_error[0]:4f}\\nNumber of features: {best_testing_error[1]}\")\n\nBest testing mse: 246.266091\nNumber of features: 195\n\n\n\nx_axis = [x for x in range(200)]\nplt.scatter(x_axis, mse_training)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean squared error (training)\")\nplt.yscale('log')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nx_axis = [x for x in range(200)]\nplt.scatter(x_axis, mse_testing)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean squared error (testing)\")\nplt.yscale('log')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nClosing Discussion\nThrough these various experiments, I learned at a high level how deep learning outpreforms traditional machine learning by taking advantage of the “double descent” phenomenon. I also learned through practice how feature matrices allow machine learning models to fit to nonlinear data, and how the use of feature maps is at the heart of deep learning, as it allows for overparameterization that can lead to optimal model performance past the interpolation threshold."
  },
  {
    "objectID": "posts/Implementing Logistic Regression/index.html",
    "href": "posts/Implementing Logistic Regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Abstract\nToday I will be showing my implementation of the logistic regression algorithm, as well as preforming various experiments to demonstrate the machine learning algorithm’s characteristics.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\n\n\nLink to Source Code\nlogistic.py\n\n\nDefining Data\nHere, we define an assortment of points that will help demonstrate how logistic regression handles classification.\n\nimport torch\ndef generate_classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = generate_classification_data(noise = 0.5)\n\n\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\nExperiments and Demonstrations\n\n\nVanilla Gradient Descent\nWithin this code, I will demonstrate how the gradient descent alogrithm converges over time to an optimal weight vector w that visually seperates a two dimensional dataset where each point belongs to 1 of 2 classes.\n\n# Written by Professor Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\ndef logistic_regression_fit(X, y, max_iterations, visualize, visual_step, beta):\n    # assert ((max_iterations / visual_step == 5) and (visual_step != 0)), \"Max_iterations / visual_step must equal 5\"\n    LR = LogisticRegression() \n    opt = GradientDescentOptimizer(LR)\n    LR.loss(X,y)\n\n    plt.rcParams[\"figure.figsize\"] = (7, 5)\n    fig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\n    markers = [\"o\", \",\"]\n    marker_map = {-1 : 0, 1 : 1}\n\n    current_ax = 0\n    n = X.size()[0]\n\n    iterations = 0\n    while iterations &lt;= max_iterations:\n        ax = axarr.ravel()[current_ax]\n        old_w = torch.clone(LR.w)\n        i = torch.randint(n, size = (1,))\n        \n        opt.step(X, y, alpha = 0.1, beta = beta)\n        loss = LR.loss(X,y).item()\n        if (iterations % visual_step == 0) and visualize:\n            plot_classification_data(X,y,ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color=\"black\")\n            ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n            # draw_line(w, -10, 10, ax, color = \"black\")\n            ax.set_title(f\"loss = {loss:.3f} iterations = {iterations}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            current_ax += 1\n        iterations+=1 \n    fig.tight_layout(h_pad=2)\nlogistic_regression_fit(X, y, 500, True, 100, 0) \n\n    \n\n\n\n\n\n\n\n\n\n\nBenefits of Momentum\nBelow, we will again call the logistic_regression_fit function, give the function a value of 0.9 for beta instead of 0. This will showcase how a vanilla gradient descent algorithm can converge much quicker due to momentum.\n\nlogistic_regression_fit(X, y, 50, True, 10, 0.9) \n\n\n\n\n\n\n\n\n\n\nMomentum Experiment Findings\nWe can see how utilizing momentum in our vanilla gradient descent algorithm can the weight vector w coverge to an optimal value much faster. This code demonstrates that while it took vanilla gradient descent 500 iterations to get a loss of 0.221, it took gradient descent with momentum only 50 iterations to get to an even lower loss of 0.208.\n\n\nOverfitting\nWe can demonstrate overfitting by creating a dataset with more dimensions that points. In this case, with enough iterations the model will be fit to perfectly capture the training data. This creates 100% training accuracy, but results in low testing accuracy, as the model’s predictions are hyper specific to the training data and do not generalize to unseen data. Within this code I demonstrate with two different datasets X_train, y_train, X_test, and y_test a 100% training accuracy (X_train, y_train) and a considerably lower testing accuracy (X_test, y_test)\n\n\nNew Data Creation Function\nIn order to illustrate overfitting, we must create two datasets (one for training, one for testing) where the data’s pattern is not easily discernible. In order to accomplish this, I create a new function, that generates n_points data points that each have p_dims dimensions, and make their label entirely random.\n\ndef new_classification_data(n_points, p_dims):\n    X = torch.randn(n_points, p_dims)\n    y = (torch.rand(n_points) &gt;= 0.5).float()\n    return X, y\n\n\n\nDemonstrating Overfitting\nNow we can call this function twice (one for training set, one for testing set) to show how logistic regression overfits when the number of dimensions greatly exceeds the number of points.\n\nX_train, y_train = new_classification_data(50, 100)\nX_test, y_test = new_classification_data(50, 100)\n\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(100):\n    opt.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\ny_train_pred = LR.predict(X_train)\ntraining_accuracy = (y_train_pred == y_train).float().mean().item()\n\nX_test_predictions = LR.predict(X_test)\ntesting_accuracy = (X_test_predictions == y_test).float().mean().item()\nprint(f\"Training accuracy: {training_accuracy}\\nTesting accuracy: {testing_accuracy}\")\n\nTraining accuracy: 1.0\nTesting accuracy: 0.41999998688697815\n\n\n\n\nResults\nWe see that the models performs very well on the training set, but much more poorly on the testing set. This demonstrates that when the dimensions of each point greatly exceeds the number of points, logistic regression overfits to the data and is unable to generalize to new data, resulting in poor predictions when encountering unseen data.\n\n\nPerformance on Empirical Data\nWe can now test our logistic regression implementation on publicly available supervised classification data. My dataset of choice will be the Breast Cancer Dataset created by M Yasser H. Within the dataset is a collection breasts with tumors labeled maligant (M) or benign (B). The features of the data set are the means of various qualities of the breasts that are predictive of identifying the tumor as being benign or maligant. In order to see these features we must first download the data.\n\n\nDownloading Data\n\nimport pandas as pd\ntrain = pd.read_csv(\"breast-cancer.csv\")\ntrain.head()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\nProcessing and Splitting the Data\nNow that we have downloaded the data and observed its features, we can now process the data to have it ready to be trained on our logistic regression model.\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\ndef prepare_data(df):\n    df = df.dropna()\n    y = np.where(df['diagnosis'] == \"M\", True, False)\n    df = df.drop(['diagnosis'], axis = 1)\n    df = df.drop(['id'], axis = 1)\n    return df, y\n\nX, y = prepare_data(train)\n\nX_train, X_test, y_train, y_test = train_test_split(\n     X, y, test_size=0.20, random_state=42)\nX_train, X_validate, y_train, y_validate = train_test_split(\n    X_train, y_train, test_size=0.20, random_state=42)\n\n(569, 30)\n\n\n\n\nModel Training\nHere, we continue to process the data and finally train the model, recording its loss over iterations for the training dataset and the validation dataset, and plot the loss over time for both datasets using two logistic regression models, one with momentum and one without. We finally assess both logistic regression models performance through their prediction accuracy on the testing dataset.\n\ndef df_to_tensor(df):\n    torch_tensor = torch.from_numpy(df.to_numpy())\n    return torch_tensor.float()\n\nX_train_tensor = df_to_tensor(X_train)\ny_train_tensor = torch.from_numpy(y_train).float()\n\nX_validate_tensor = df_to_tensor(X_validate)\ny_validate_tensor = torch.from_numpy(y_validate).float()\n\nX_test_tensor = df_to_tensor(X_test)\ny_test_tensor = torch.from_numpy(y_test).float()\n\n# Logistic regression model without momentum\nLR = LogisticRegression() \nLR_momentum = LogisticRegression()\n\n# Logistic regression model with momentum\nopt = GradientDescentOptimizer(LR)\nopt_momentum = GradientDescentOptimizer(LR_momentum)\n\n# Tracking the output of the loss function during trainig and when evaluting training using validation dataset\ntraining_loss, training_loss_momentum = [], []\nvalidation_loss, validation_loss_momentum = [], []\nnum_iterations = 10000\nrecord_num = 1000\n\n# Here, we do the training for the Logistic Regression models (one with momentum and one without) and track the loss\n# over the training period\nfor i in range(num_iterations):\n    opt.step(X_train_tensor, y_train_tensor, alpha = 0.1, beta = 0.0)\n    opt_momentum.step(X_train_tensor, y_train_tensor, alpha = 0.1, beta = 0.9)\n  \n    if(i % record_num == 0):\n        training_loss.append(LR.loss(X_train_tensor, y_train_tensor).item())\n        training_loss_momentum.append(LR_momentum.loss(X_train_tensor, y_train_tensor).item())\n\n        validation_loss.append(LR.loss(X_validate_tensor, y_validate_tensor).item())\n        validation_loss_momentum.append(LR_momentum.loss(X_validate_tensor, y_validate_tensor).item())\n\n# Plot training and validation loss\nx_axis = list(range(0, num_iterations, record_num))\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_axis, training_loss, label=\"Training (no momentum)\")\nplt.plot(x_axis, training_loss_momentum, label=\"Training (momentum)\")\nplt.plot(x_axis, validation_loss, label=\"Validation (no momentum)\", linestyle='--')\nplt.plot(x_axis, validation_loss_momentum, label=\"Validation (momentum)\", linestyle='--')\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss over Time\")\nplt.legend()\nplt.show()\n\ndef accuracy(y_pred, y_true):\n    predicted = (y_pred &gt;= 0.5).float()\n    correct = (predicted == y_true).float().sum()\n    return (correct / y_true.shape[0]).item()\n\ntest_preds = LR.predict(X_test_tensor)\ntest_preds_momentum = LR_momentum.predict(X_test_tensor)\n\ntest_loss = LR.loss(X_test_tensor, y_test_tensor).item()\ntest_loss_momentum = LR_momentum.loss(X_test_tensor, y_test_tensor).item()\ntest_accuracy = accuracy(test_preds, y_test_tensor)\ntest_accuracy_momentum = accuracy(test_preds_momentum, y_test_tensor)\n                    \nprint(f\"Test loss (no momentum): {test_loss:.4f}\")\nprint(f\"Test loss (momentum): {test_loss_momentum:.4f}\")\n\nprint(f\"Test accuracy (no momentum): {test_accuracy:.4f}\")\nprint(f\"Test accuracy (momentum): {test_accuracy_momentum:.4f}\")\n\n\n\n\n\n\n\n\nTest loss (no momentum): 1.4139\nTest loss (momentum): 0.5625\nTest accuracy (no momentum): 0.9123\nTest accuracy (momentum): 0.9649\n\n\n\n\nResults\nWe can see through the training loss, validation loss, and test accuracy with and without momentum that utilizing momentum gives us faster convergence on average, however can lead to large leaps in the value of loss the loss function in the positive and negative direction. Overall, the logisistic regression algorithm performed very well on this binary classification task with a testing accuracy of 0.9123 without momentum and 0.9649 with momentum.\n\n\nConclusion\nDuring the creation of this blog post, I learned a lot about the implementation of the Logistic Regression algorithm, and through experimentation learned that Logistic Regressionw with momentum leads to faster covergence and that the algorithm overfits when dimensions exceed number of data points. Through using my implementation on a real dataset, I proved the validity of Logistic Regression’s usefulness on real binary classification problems."
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "Today, we will be using the PUMS dataset to predict employment of members in the state of Michigan that are within the dataset, as well as auditing for racial bias. The process we will be using is first implementing a SVC to predict employment status based on features excluding race and the target variable (employment status), then evaluating our findings based on statistical definitions of fairness, such as calibration, error rate balance, and statistical parity of each racial group (white, black, or other). Through this process, I evaluated my model as passing both approximate cabliration and error rate balance tests but failing the statistical parity test, with white people having a higher rate of being predicted to be employed than black people or other people of a self-identified racial identity. Overall, I note in the conclusion the predicted impact of implementing this predictor model at large and note potential issues/uncomforabilites."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#overall-measures",
    "href": "posts/Auditing Bias/index.html#overall-measures",
    "title": "Auditing Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\n\nWhat is the overall accuracy of the model?\nWhat is the positive predictive value (PPV) of the model?\nWhat are the overall false negative and flase positive rates (FNP and FPR) for the model?\n\n\nOverall Acurracy of Model\nTo get the overall accuracy of the model, get a boolean vector through (y_hat == y_test) which will give an array of truth or false values corresponding to the equality of the prediction and the actual value. To get the percentage of the correct predictions in the boolean vector, we take the mean with the mean() function.\n\nmodel_accuracy = (y_hat == y_test).mean()\nmodel_accuracy\n\nnp.float64(0.825940454636894)\n\n\nThe overall accuracy of the model is approximately 82.5%\n\n\nPPV of the Model\nTo get the positive predictive value of the model, we divide the number of true positives (TP) by the total number of positive predictions (TP + FP)\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_array = confusion_matrix(y_test, y_hat)\nconfusion_array\n\narray([[8744, 2269],\n       [1192, 7679]])\n\n\n\nTP = confusion_array[0][0]\nFN = confusion_array[0][1]\nFP = confusion_array[1][0]\nTN = confusion_array[1][1]\n\nPPV = TP / (TP+FP)\nPPV\n\nnp.float64(0.8800322061191627)\n\n\n\n\nFPR and FNR of the Model\nSimilarly, we can use the confusion matrix to get the FPR and FNR of the model\n\n# False positive rate calculation\nFPR = FP / (FP + TN)\nFPR\n\nnp.float64(0.1343704204711983)\n\n\n\n# False negative rate calculation\nFNR = FN / (FN + TP)\nFNR\n\nnp.float64(0.2060292381730682)"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#by-group-measures",
    "href": "posts/Auditing Bias/index.html#by-group-measures",
    "title": "Auditing Bias",
    "section": "By-Group Measures",
    "text": "By-Group Measures\nNow that we have observed the overall accuracy of the model, we now concern ourselves with the accuracy of the model as it pertains to the racial groups (white, black, other) that exist within the dataset. Below we ask a few questions:\n\nWhat is the accuracy of your model on each subgroup\nWhat is the PPV of your model on each subgroup?\nWhat are the FNR and FPR on each subgroup?\n\nTo answer question 1, we can make a summary table displaying the accuracy of the model on each subgroup\n\naccuracy_group_1 = (y_hat[group_test == 1] == y_test[group_test == 1]).mean()\naccuracy_group_2 = (y_hat[group_test == 2] == y_test[group_test == 2]).mean()\naccuracy_group_3 = (y_hat[group_test &gt;= 3] == y_test[group_test &gt;= 3]).mean()\n\nsubgroup_accuracy = pd.Series({\"White\": accuracy_group_1,\"Black\": accuracy_group_2, \"Other\": accuracy_group_3})\nsub_acc_df = pd.DataFrame(subgroup_accuracy, columns=[\"Accuracy\"])\nsub_acc_df\n\n\n\n\n\n\n\n\nAccuracy\n\n\n\n\nWhite\n0.827000\n\n\nBlack\n0.816878\n\n\nOther\n0.824841\n\n\n\n\n\n\n\nTo answer question 2, we can return to the idea of confusion matrices, making one for each subgroup and calculating the PPV as TP (True Positive Predictions) / TP + FP (False Positive Predictions).\n\nwhite_confusion_array = confusion_matrix((y_test[group_test == 1]), y_hat[group_test == 1])  \nblack_confusion_array = confusion_matrix((y_test[group_test == 2]), y_hat[group_test == 2])\nother_confusion_array = confusion_matrix((y_test[group_test == 3]), y_hat[group_test == 3])\n\nwhite_PPV = white_confusion_array[0][0] / (white_confusion_array[0][0] + white_confusion_array[0][1])\nblack_PPV = black_confusion_array[0][0] / (black_confusion_array[0][0] + black_confusion_array[0][1])\nother_PPV = other_confusion_array[0][0] / (other_confusion_array[0][0] + other_confusion_array[0][1])\n\ngroup_PPV = pd.Series({\"white\": white_PPV,\"black\": black_PPV, \"other\": other_PPV})\ngroup_PPV_df = pd.DataFrame(group_PPV, columns=[\"PPV\"])\ngroup_PPV_df\n\n\n\n\n\n\n\n\nPPV\n\n\n\n\nwhite\n0.791346\n\n\nblack\n0.801043\n\n\nother\n0.743590\n\n\n\n\n\n\n\nTo answer question 3, we can take a simiar approach as shown above, only switching the values that we are taking from the confusion matrix\n\nwhite_confusion_array = confusion_matrix((y_test[group_test == 1]), y_hat[group_test == 1])  \nblack_confusion_array = confusion_matrix((y_test[group_test == 2]), y_hat[group_test == 2])\nother_confusion_array = confusion_matrix((y_test[group_test == 3]), y_hat[group_test == 3])\n\nwhite_FNR = white_confusion_array[1][0] / (white_confusion_array[0][0] + white_confusion_array[1][0])\nwhite_FPR = white_confusion_array[0][1] / (white_confusion_array[0][1] + white_confusion_array[1][1])\n\nblack_FNR = black_confusion_array[1][0] / (black_confusion_array[0][0] + black_confusion_array[1][0])\nblack_FPR = black_confusion_array[0][1] / (black_confusion_array[0][1] + black_confusion_array[1][1])\n\nother_FNR = other_confusion_array[1][0] / (other_confusion_array[0][0] + other_confusion_array[1][0])\nother_FPR = other_confusion_array[0][1] / (other_confusion_array[0][1] + other_confusion_array[1][1])\n\ngroup_FNR = pd.Series({\"white\": white_FNR,\"black\": black_FNR, \"other\": other_FNR})\ngroup_FPR = pd.Series({\"white\": white_FPR,\"black\": black_FPR, \"other\": other_FPR})\n\n\n\ngroup_FPR_FNR_df = pd.DataFrame({\"FNR\": group_FNR, \"FPR\": group_FPR})\ngroup_FPR_FNR_df\n\n\n\n\n\n\n\n\nFNR\nFPR\n\n\n\n\nwhite\n0.122824\n0.220930\n\n\nblack\n0.100488\n0.290609\n\n\nother\n0.093750\n0.322581"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#bias-measures",
    "href": "posts/Auditing Bias/index.html#bias-measures",
    "title": "Auditing Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nNow that we have evaluted the overall accuracy and the accuracy within groups of the model, we now use statistical definitions of fairness to ascertain a sense of the models “fairness”. Below, we ask a few questions:\n\nIs the model approximately calibrated?\nDoes the model satisfy approximate error rate balance?\nDoes the model satisfy statistical parity?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Sparse Kernel Machines\n\n\n\n\n\nA blog post exploring kernelized logisitic regression\n\n\n\n\n\nMay 16, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nNewtons’s Method and Adam\n\n\n\n\n\nA blog post exploring Newton’s Method of optimizing the logistic regression algorithm\n\n\n\n\n\nMay 12, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nDouble Descent\n\n\n\n\n\nA blog post exploring overfitting and the double descent phenomenon\n\n\n\n\n\nMay 12, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nDiscussion of the efficacy of quantitative methods in evaluating the fairness of machine learning algorithms.\n\n\n\n\n\nApr 16, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\nA blog post implementing Logistic Regression\n\n\n\n\n\nApr 6, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\nImplementing and Experimenting with the Perceptron Algorithm\n\n\n\n\n\nMar 31, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nUsing the PUMS dataset to predict employment and audit for racial bias.\n\n\n\n\n\nMar 12, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nAn introduction into automated decision making systems with machine learning and their societal impacts\n\n\n\n\n\nMar 5, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins Blog Post\n\n\n\n\n\nUsing the Palmer Penguins dataset to make predictions on species of penguins\n\n\n\n\n\nFeb 20, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]