[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Cameron Hudson’s Machine Learning Blog",
    "section": "",
    "text": "A blog to explore machine learning!"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Penguin Blog Post/index.html",
    "href": "posts/Penguin Blog Post/index.html",
    "title": "Palmer Penguins Blog Post",
    "section": "",
    "text": "Welcome! Today, I am going to demonstrate how to train a Logistic Regression model in order to make predictions on species of penguins using the Palmer Penguins dataset. We will cover how to find visualize and select good qualitative and quantiative predictors, bruteforce feature selection, how to plot decision regions, confusion matrices in attempt to show a comprehensive guide of the process of making a predictor model with a dataset.\n\nAcquring the Data\nWe are first going to import two popular data science python libraries, pandas and numpy. After this, we are going to get our Palmers Penguins dataset from the url listed below, and use pandas to convert the csv file into a DataFrame which will be stored in the variable train.\n\nimport pandas as pd\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nLet’s see 5 of the pieces of data we are working with using the head() method.\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\nData Preperation\nWe are now going to prepare the data. Here we drop columns that are likely unhelpful in making a prediction on the species of penguins, specifying axis = 1 to delete the column instead of the index. We then remove all rows were the value of df[“Sex”] is undefined or “.”. We use the method dropna() to drop all the rows that contain missing values. We then use the method fit() from the LabelEncoder object to pick numerical values from qualitative values. For example, our penguin species are “Gentoo”, “Chinstrap”, and “Adelie” which could be encoded as 0, 1, and 2. Now that we have encoded the values of the “Species” column, we create a column that uses that encoding with le.transform, and we want to use the encoded values on the “Species” column, so we will do le.transform(df[“Species”]) and store this new column in y. We will then drop the original Species column as we will not need it for our prediction. Finally, we do pd.get_dummies(df) which will convert all categorical variables that only have 2 unique values to a 0 or a 1. With this, we have prepared are data and are ready to use it to make predictions on species of Penguins.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nfor i, c in enumerate(le.classes_):\n    print(f\"Class number {i} represents {c} penguins.\")\n\nX_train, y_train = prepare_data(train)\n\nClass number 0 represents Adelie Penguin (Pygoscelis adeliae) penguins.\nClass number 1 represents Chinstrap penguin (Pygoscelis antarctica) penguins.\nClass number 2 represents Gentoo penguin (Pygoscelis papua) penguins.\n\n\nHere, we also enumerated through the classes made by the LabelEncoder and used a fstring to cleanly show which number encoding referes to which penguin.\nNow we can see that we have removed unnessecary columns, dropped rows with empty values, and changed qualitative columns to columns of True or False (1 or 0) values:\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nData Visualizations\nHere we are going to visualize the relationship between some quantative data to get a feel for what features might be useful for our predictor model.\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nle.classes_ = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nspecies_names = [le.classes_[i] for i in y_train]\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(X_train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(X_train, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\", ax = ax[1], hue = species_names, palette=\"Set2\")\n\n\n\n\n\n\n\n\nHere we can see that the relationship between Flipper Length and Body Mass is good for distinguishing between Gentoo penguins and the other two species (data shows that Gentoo penugins on average have a greater body mass and flipper length than the two other species), however it is not so good for dintinguishing between the Adelie and Chinstrap penguin species.\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\np1 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[0], color = \"darkgrey\")\np2 = sns.scatterplot(X_train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[1], hue = species_names, palette=\"Set2\")\n\n\n\n\n\n\n\n\nWe can see here that utilizing the culmen length and the flipper length gives rather distinct groups where each species of penguins resides. While some points overlap, showing that these variables don’t 100% identify a penguin’s species, it is certainly better at identifying chinstrap and adelie penguins from the last set of variables.\n\n\nSummary Tables and Aggregate Values\nBeyond plotting which is good for visualizing quantiative features and there usefulness, we can use aggregate values to gain more information on quantiative features such as sex, island, and clutch completion.\n\ntrain.groupby(\"Clutch Completion\").size() / X_train.shape[0]\n\nClutch Completion\nNo     0.117188\nYes    0.957031\ndtype: float64\n\n\n\ntrain.groupby(\"Sex\").size() / X_train.shape[0]\n\nSex\n.         0.003906\nFEMALE    0.519531\nMALE      0.511719\ndtype: float64\n\n\n\ntrain.groupby(\"Island\").size() / X_train.shape[0]\n\nIsland\nBiscoe       0.511719\nDream        0.398438\nTorgersen    0.164062\ndtype: float64\n\n\nHere we can see the percentage of all the penguins split between the various qualitative features (Sex, Island, and Clutch Completion). With this we can see that clutch completion will probably not help us determine the species of a particular penguin as for 95% of the penguins in the dataset Clutch_Completion is true. On the other hand, it seems that half of the penguins are male and the other half female, and that a penguin’s location on one of the three islands is also quite varied, making these qualiative features more useful for predictions than cluch completion.\n\n\nFeature Selection\nOur current task now is to find three of these features listed where 2 are quantiative and 1 are quantitative that will predict with high accuracy the species of Penguin (either Gentoo, Chinstrap, or Adelie).\n\nprint(X_train.columns)\n\nIndex(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)',\n       'Island_Biscoe', 'Island_Dream', 'Island_Torgersen',\n       'Stage_Adult, 1 Egg Stage', 'Clutch Completion_No',\n       'Clutch Completion_Yes', 'Sex_FEMALE', 'Sex_MALE'],\n      dtype='object')\n\n\nBecause we have a relatively small amount of features, we are going to train a logistic regression model on every distinct combination of 2 quantitative feautures and 1 qualitative feature, and keep track in the variable max_score what combination of features (stored in best_features) achieves the highest acurracy.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\ntest_cols = X_train\nbest_features = ['None']\nmax_score = 0\n\nLR = LogisticRegression(max_iter=20000)\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    test_cols = X_train[cols]\n    m = LR.fit(test_cols, y_train)\n    score = LR.score(test_cols, y_train)\n    if score &gt; max_score:\n      max_score = score\n      best_features = cols\nbest_features = best_features[2:] + best_features[0:2]\nm = LR.fit(X_train[best_features], y_train)\nprint(max_score, best_features)\n\n0.99609375 ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\nWith this, we have discovered that training a logistic regression model on the qualitative feature Sex (Male or Female) and the quantitative features Culmen Length (mm) and Culmen Depth (mm) gives the highest prediction accurracy among all the other 3 feature combinations with a accuracy of 99.5% on the training data.\n\n\nTesting the model on unseen data\nWe have achieved high accuracy using the training data, but can we achieve similar results using a different subset of data?\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_features], y_test)\n\n0.9852941176470589\n\n\nWe can see that the accuracy of our model on new data is only slightly worse at 98.5%.\n\n\nPlotting Decision Regions\nWe can more explicitly display how our model is deciding the species of penguins with our selected features with this plot_regions function.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nWe are first going to run this using the training data:\n\nplot_regions(LR, X_train[best_features], y_train)\n\n\n\n\n\n\n\n\nThen, we are going to run it through the testing data:\n\nplot_regions(LR, X_test[best_features], y_test)\n\n\n\n\n\n\n\n\nHere we can see that the distinct regions that model predicted each of the 3 species of penguins would be located at mostly correlates with the actual positions and species of the penguin data points, giving a more visual representation of our accuracy value of 99.6% on training data and 98.5% on testing data.\n\n\nConfusion Matrix\nUsing the confusion_matrix function from sklearn, we can create a matrix that shows the accurate and inaccurate predictions of our model.\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = LR.predict(X_test[best_features],)\nC = confusion_matrix(y_test, y_test_pred)\nC\n\narray([[31,  0,  0],\n       [ 0, 10,  1],\n       [ 0,  0, 26]])\n\n\nHere, on the diagonal of the matrix is the correct guesses, with each row corresponding to each possible label prediction (Chinstrap, Adelie, or Gentoo). We can make the output of our confusion matrix more appearent by returning to our LabelEncoder and looping through its contents:\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 10 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 1 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nWe can see that the only error the model made on the testing data was that it misidentified a Chinstrap penguin for a Gentoo penguin. This correlates with the plot_regions output using the training data, as the points that represent Chinstrap penguins were often close or in the region that predicts a penguin will be a Gentoo penguin.\n\n\nClosing\nWith this, we have predicted with high accuracy the species of penguins within the Palmer Penguins dataset. Through the process of feature selection and plotting data we found that the sex, culmen length and depth of penguins within the dataset are the best features to predict the particular species of that penguin. This exercise has taught me the general workflow of creating a predictor model with sklearn, numpy and pandas from start to finish. I also learned techniques of plotting data (scatterplots, desicion regions) what a confusion matrix is and how to interpret it, and the process of feature selection."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Palmer Penguins Blog Post\n\n\n\n\n\nUsing the Palmer Penguins dataset to make predictions on species of penguins\n\n\n\n\n\nFeb 20, 2025\n\n\nCameron Hudson\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]